{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f0aa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sanket\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sanket\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries:\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ddcebc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>gujarati fraud kyu hotey hai? pnbscam</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9161</th>\n",
       "      <td>saala idhar 7 lakh k car loan k liye bank chooos leti hai aur waha log crores ka fraud karte hai bada socho niravmodi pnbfraud</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9162</th>\n",
       "      <td>mitron nirav modi ji ka relation india k kaun se rich industrialist parivaar se hai ? batao zara ? pnbfraud</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9163</th>\n",
       "      <td>or yahn ek month k education loan ki emi pay ni karo to call aa jata hai pnbfraud  so sad</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9164</th>\n",
       "      <td>jis bank me 500 rupye niklwane k liye ghnto khda rehna pdta hai unhone aaj btaya k $7 bn koi dokhe se idhr udhr kr gya pnb fraud this makes me angry</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                     tweets  \\\n",
       "9160                                                                                                                  gujarati fraud kyu hotey hai? pnbscam   \n",
       "9161                         saala idhar 7 lakh k car loan k liye bank chooos leti hai aur waha log crores ka fraud karte hai bada socho niravmodi pnbfraud   \n",
       "9162                                            mitron nirav modi ji ka relation india k kaun se rich industrialist parivaar se hai ? batao zara ? pnbfraud   \n",
       "9163                                                             or yahn ek month k education loan ki emi pay ni karo to call aa jata hai pnbfraud  so sad    \n",
       "9164  jis bank me 500 rupye niklwane k liye ghnto khda rehna pdta hai unhone aaj btaya k $7 bn koi dokhe se idhr udhr kr gya pnb fraud this makes me angry    \n",
       "\n",
       "      labels  \n",
       "9160       3  \n",
       "9161       3  \n",
       "9162       3  \n",
       "9163       2  \n",
       "9164       3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Code Mixed Hindi-English tweets.xlsx')\n",
    "df.drop(['id'],axis=1, inplace=True)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None) # to display full length text of column.\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df2e90",
   "metadata": {},
   "source": [
    "# Labels are as follows:\n",
    "0 - No emotions,\n",
    "1 - Happy,\n",
    "2 - Sad,\n",
    "3 - Angry,\n",
    "4 - Fear,\n",
    "5 - Disgust,\n",
    "6 - Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7d62f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (9165, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1892\n",
       "3    1763\n",
       "2    1529\n",
       "1    1226\n",
       "5    1147\n",
       "6    1049\n",
       "4     559\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset shape: \", df.shape)\n",
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1408b36",
   "metadata": {},
   "source": [
    "# 1. Models without Removing anything:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace98bbc",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b9dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "X = df['tweets']\n",
    "y = df['labels']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c185974",
   "metadata": {},
   "source": [
    "### Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f16534d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  19470\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,1) - unigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,1), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "# Transforming the data using TD-IDF Vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a1017d",
   "metadata": {},
   "source": [
    "### Base Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a705d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dictionary with four models with some parameters:\n",
    "model_params = {\n",
    "    \n",
    "    'SVC' :{\n",
    "        'model' : SVC(),\n",
    "        'params' : {\n",
    "            'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01], 'kernel': ['rbf','linear','poly','sigmoid']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'logistics_regression' :{\n",
    "        'model' : LogisticRegression(solver = 'lbfgs', multi_class = 'auto'),\n",
    "        'params' : {\n",
    "            'C' : [0.1, 1, 20, 40, 60, 80, 100], 'solver' : ['lbfgs', 'liblinear']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'random_forest' :{\n",
    "        'model' : RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            'n_estimators' : [80,85,90,95,100], \n",
    "            'max_depth':[20,30,None], 'criterion':['gini','entropy']\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90cc8375",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[[323   2   5  19   3   1   8]\n",
      " [ 43 171  13  14   0   1   0]\n",
      " [ 53  11 219  24   0   1   2]\n",
      " [ 58   5  13 259   0   7   4]\n",
      " [ 14   3   6   6  70   3   2]\n",
      " [ 35   3   4  21   1 175   0]\n",
      " [ 45   4   4  14   2   1 161]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.89      0.69       361\n",
      "           1       0.86      0.71      0.78       242\n",
      "           2       0.83      0.71      0.76       310\n",
      "           3       0.73      0.75      0.74       346\n",
      "           4       0.92      0.67      0.78       104\n",
      "           5       0.93      0.73      0.82       239\n",
      "           6       0.91      0.70      0.79       231\n",
      "\n",
      "    accuracy                           0.75      1833\n",
      "   macro avg       0.82      0.74      0.76      1833\n",
      "weighted avg       0.79      0.75      0.76      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[[323   1   9  17   2   1   8]\n",
      " [ 48 163  17  10   0   2   2]\n",
      " [ 57  13 214  20   0   3   3]\n",
      " [ 61   6  17 250   0  10   2]\n",
      " [ 19   2   6   3  70   2   2]\n",
      " [ 40   2   6  16   1 174   0]\n",
      " [ 54   4   3  14   1   3 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.89      0.67       361\n",
      "           1       0.85      0.67      0.75       242\n",
      "           2       0.79      0.69      0.74       310\n",
      "           3       0.76      0.72      0.74       346\n",
      "           4       0.95      0.67      0.79       104\n",
      "           5       0.89      0.73      0.80       239\n",
      "           6       0.90      0.66      0.76       231\n",
      "\n",
      "    accuracy                           0.73      1833\n",
      "   macro avg       0.81      0.72      0.75      1833\n",
      "weighted avg       0.78      0.73      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "RandomForestClassifier()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[[329   5   6   7   3   0  11]\n",
      " [ 47 177  14   1   0   1   2]\n",
      " [ 68  17 208   9   1   0   7]\n",
      " [ 87  12  13 222   0   7   5]\n",
      " [ 19   3   3   0  73   3   3]\n",
      " [ 41   6  10   6   1 175   0]\n",
      " [ 46  10   5   4   1   2 163]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.91      0.66       361\n",
      "           1       0.77      0.73      0.75       242\n",
      "           2       0.80      0.67      0.73       310\n",
      "           3       0.89      0.64      0.75       346\n",
      "           4       0.92      0.70      0.80       104\n",
      "           5       0.93      0.73      0.82       239\n",
      "           6       0.85      0.71      0.77       231\n",
      "\n",
      "    accuracy                           0.73      1833\n",
      "   macro avg       0.81      0.73      0.75      1833\n",
      "weighted avg       0.79      0.73      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "Wall time: 11min 5s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.751773</td>\n",
       "      <td>{'C': 1, 'gamma': 1, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistics_regression</td>\n",
       "      <td>0.734315</td>\n",
       "      <td>{'C': 1, 'solver': 'lbfgs'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.734861</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 95}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  best_score  \\\n",
       "0                   SVC    0.751773   \n",
       "1  logistics_regression    0.734315   \n",
       "2         random_forest    0.734861   \n",
       "\n",
       "                                                    best_params  \n",
       "0                      {'C': 1, 'gamma': 1, 'kernel': 'linear'}  \n",
       "1                                   {'C': 1, 'solver': 'lbfgs'}  \n",
       "2  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 95}  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# implemented GridSearchCV for four models using a loop and a previously created dictionary\n",
    "# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "    print(mp['model'])\n",
    "    print('\\nFitting...')\n",
    "    best_model = clf.fit(X_train, y_train)                      # Training the model\n",
    "    clf_pred = best_model.predict(X_test)                       # Predicting the results\n",
    "    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n",
    "    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n",
    "    scores.append({                                             # Appending results to 'scores' list\n",
    "        'model' : model_name,\n",
    "        'best_score' : best_model.score(X_test, y_test),\n",
    "        'best_params' : clf.best_params_\n",
    "    })\n",
    "    print('\\nScore is appended.\\n')\n",
    "    \n",
    "# Creating data frame with model, best scores and best params:\n",
    "res = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a8509b",
   "metadata": {},
   "source": [
    "# 2. Models after removing Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e0b942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Code Mixed Hindi-English tweets.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523143f1",
   "metadata": {},
   "source": [
    "### Removing Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3cd79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hinglish stopwords which contains 1036 words from both English and Hindi languages\n",
    "# Source: https://github.com/TrigonaMinima/HinglishNLP/blob/master/data/assets/stop_hinglish\n",
    "\n",
    "stopwordlist = ['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab', 'abbe', 'abbey', 'abe', 'abhi', 'able', 'about', 'above', 'accha', 'according', 'accordingly', 'acha', 'achcha', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'agar', 'ain', 'aint', \"ain't\", 'aisa', 'aise', 'aisi', 'alag', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'andar', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'ap', 'apan', 'apart', 'apna', 'apnaa', 'apne', 'apni', 'appear', 'are', 'aren', 'arent', \"aren't\", 'around', 'arre', 'as', 'aside', 'ask', 'asking', 'at', 'aur', 'avum', 'aya', 'aye', 'baad', 'baar', 'bad', 'bahut', 'bana', 'banae', 'banai', 'banao', 'banaya', 'banaye', 'banayi', 'banda', 'bande', 'bandi', 'bane', 'bani', 'bas', 'bata', 'batao', 'bc', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'bhai', 'bheetar', 'bhi', 'bhitar', 'bht', 'bilkul', 'bohot', 'bol', 'bola', 'bole', 'boli', 'bolo', 'bolta', 'bolte', 'bolti', 'both', 'brief', 'bro', 'btw', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain', 'certainly', 'chahiye', 'chaiye', 'chal', 'chalega', 'chhaiye', 'clearly', \"c'mon\", 'com', 'come', 'comes', 'could', 'couldn', 'couldnt', \"couldn't\", 'd', 'de', 'dede', 'dega', 'degi', 'dekh', 'dekha', 'dekhe', 'dekhi', 'dekho', 'denge', 'dhang', 'di', 'did', 'didn', 'didnt', \"didn't\", 'dijiye', 'diya', 'diyaa', 'diye', 'diyo', 'do', 'does', 'doesn', 'doesnt', \"doesn't\", 'doing', 'done', 'dono', 'dont', \"don't\", 'doosra', 'doosre', 'down', 'downwards', 'dude', 'dunga', 'dungi', 'during', 'dusra', 'dusre', 'dusri', 'dvaara', 'dvara', 'dwaara', 'dwara', 'each', 'edu', 'eg', 'eight', 'either', 'ek', 'else', 'elsewhere', 'enough', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'fir', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forth', 'four', 'from', 'further', 'furthermore', 'gaya', 'gaye', 'gayi', 'get', 'gets', 'getting', 'ghar', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'good', 'got', 'gotten', 'greetings', 'haan', 'had', 'hadd', 'hadn', 'hadnt', \"hadn't\", 'hai', 'hain', 'hamara', 'hamare', 'hamari', 'hamne', 'han', 'happens', 'har', 'hardly', 'has', 'hasn', 'hasnt', \"hasn't\", 'have', 'haven', 'havent', \"haven't\", 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', \"here's\", 'hereupon', 'hers', 'herself', \"he's\", 'hi', 'him', 'himself', 'his', 'hither', 'hm', 'hmm', 'ho', 'hoga', 'hoge', 'hogi', 'hona', 'honaa', 'hone', 'honge', 'hongi', 'honi', 'hopefully', 'hota', 'hotaa', 'hote', 'hoti', 'how', 'howbeit', 'however', 'hoyenge', 'hoyengi', 'hu', 'hua', 'hue', 'huh', 'hui', 'hum', 'humein', 'humne', 'hun', 'huye', 'huyi', 'i', \"i'd\", 'idk', 'ie', 'if', \"i'll\", \"i'm\", 'imo', 'in', 'inasmuch', 'inc', 'inhe', 'inhi', 'inho', 'inka', 'inkaa', 'inke', 'inki', 'inn', 'inner', 'inse', 'insofar', 'into', 'inward', 'is', 'ise', 'isi', 'iska', 'iskaa', 'iske', 'iski', 'isme', 'isn', 'isne', 'isnt', \"isn't\", 'iss', 'isse', 'issi', 'isski', 'it', \"it'd\", \"it'll\", 'itna', 'itne', 'itni', 'itno', 'its', \"it's\", 'itself', 'ityaadi', 'ityadi', \"i've\", 'ja', 'jaa', 'jab', 'jabh', 'jaha', 'jahaan', 'jahan', 'jaisa', 'jaise', 'jaisi', 'jata', 'jayega', 'jidhar', 'jin', 'jinhe', 'jinhi', 'jinho', 'jinhone', 'jinka', 'jinke', 'jinki', 'jinn', 'jis', 'jise', 'jiska', 'jiske', 'jiski', 'jisme', 'jiss', 'jisse', 'jitna', 'jitne', 'jitni', 'jo', 'just', 'jyaada', 'jyada', 'k', 'ka', 'kaafi', 'kab', 'kabhi', 'kafi', 'kaha', 'kahaa', 'kahaan', 'kahan', 'kahi', 'kahin', 'kahte', 'kaisa', 'kaise', 'kaisi', 'kal', 'kam', 'kar', 'kara', 'kare', 'karega', 'karegi', 'karen', 'karenge', 'kari', 'karke', 'karna', 'karne', 'karni', 'karo', 'karta', 'karte', 'karti', 'karu', 'karun', 'karunga', 'karungi', 'kaun', 'kaunsa', 'kayi', 'kch', 'ke', 'keep', 'keeps', 'keh', 'kehte', 'kept', 'khud', 'ki', 'kin', 'kine', 'kinhe', 'kinho', 'kinka', 'kinke', 'kinki', 'kinko', 'kinn', 'kino', 'kis', 'kise', 'kisi', 'kiska', 'kiske', 'kiski', 'kisko', 'kisliye', 'kisne', 'kitna', 'kitne', 'kitni', 'kitno', 'kiya', 'kiye', 'know', 'known', 'knows', 'ko', 'koi', 'kon', 'konsa', 'koyi', 'krna', 'krne', 'kuch', 'kuchch', 'kuchh', 'kul', 'kull', 'kya', 'kyaa', 'kyu', 'kyuki', 'kyun', 'kyunki', 'lagta', 'lagte', 'lagti', 'last', 'lately', 'later', 'le', 'least', 'lekar', 'lekin', 'less', 'lest', 'let', \"let's\", 'li', 'like', 'liked', 'likely', 'little', 'liya', 'liye', 'll', 'lo', 'log', 'logon', 'lol', 'look', 'looking', 'looks', 'ltd', 'lunga', 'm', 'maan', 'maana', 'maane', 'maani', 'maano', 'magar', 'mai', 'main', 'maine', 'mainly', 'mana', 'mane', 'mani', 'mano', 'many', 'mat', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'mein', 'mera', 'mere', 'merely', 'meri', 'might', 'mightn', 'mightnt', \"mightn't\", 'mil', 'mjhe', 'more', 'moreover', 'most', 'mostly', 'much', 'mujhe', 'must', 'mustn', 'mustnt', \"mustn't\", 'my', 'myself', 'na', 'naa', 'naah', 'nahi', 'nahin', 'nai', 'name', 'namely', 'nd', 'ne', 'near', 'nearly', 'necessary', 'neeche', 'need', 'needn', 'neednt', \"needn't\", 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nhi', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nope', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'par', 'pata', 'pe', 'pehla', 'pehle', 'pehli', 'people', 'per', 'perhaps', 'phla', 'phle', 'phli', 'placed', 'please', 'plus', 'poora', 'poori', 'provides', 'pura', 'puri', 'q', 'que', 'quite', 'raha', 'rahaa', 'rahe', 'rahi', 'rakh', 'rakha', 'rakhe', 'rakhen', 'rakhi', 'rakho', 'rather', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'rehte', 'rha', 'rhaa', 'rhe', 'rhi', 'ri', 'right', 's', 'sa', 'saara', 'saare', 'saath', 'sab', 'sabhi', 'sabse', 'sahi', 'said', 'sakta', 'saktaa', 'sakte', 'sakti', 'same', 'sang', 'sara', 'sath', 'saw', 'say', 'saying', 'says', 'se', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'shan', 'shant', \"shan't\", 'she', \"she's\", 'should', 'shouldn', 'shouldnt', \"shouldn't\", \"should've\", 'si', 'since', 'six', 'so', 'soch', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'still', 'sub', 'such', 'sup', 'sure', 't', 'tab', 'tabh', 'tak', 'take', 'taken', 'tarah', 'teen', 'teeno', 'teesra', 'teesre', 'teesri', 'tell', 'tends', 'tera', 'tere', 'teri', 'th', 'tha', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that's\", 'the', 'theek', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', \"there's\", 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'thi', 'thik', 'thing', 'think', 'thinking', 'third', 'this', 'tho', 'thoda', 'thodi', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'three', 'through', 'throughout', 'thru', 'thus', 'tjhe', 'to', 'together', 'toh', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'true', 'truly', 'try', 'trying', 'tu', 'tujhe', 'tum', 'tumhara', 'tumhare', 'tumhari', 'tune', 'twice', 'two', 'um', 'umm', 'un', 'under', 'unhe', 'unhi', 'unho', 'unhone', 'unka', 'unkaa', 'unke', 'unki', 'unko', 'unless', 'unlikely', 'unn', 'unse', 'until', 'unto', 'up', 'upar', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'usi', 'using', 'uska', 'uske', 'usne', 'uss', 'usse', 'ussi', 'usually', 'vaala', 'vaale', 'vaali', 'vahaan', 'vahan', 'vahi', 'vahin', 'vaisa', 'vaise', 'vaisi', 'vala', 'vale', 'vali', 'various', 've', 'very', 'via', 'viz', 'vo', 'waala', 'waale', 'waali', 'wagaira', 'wagairah', 'wagerah', 'waha', 'wahaan', 'wahan', 'wahi', 'wahin', 'waisa', 'waise', 'waisi', 'wala', 'wale', 'wali', 'want', 'wants', 'was', 'wasn', 'wasnt', \"wasn't\", 'way', 'we', \"we'd\", 'well', \"we'll\", 'went', 'were', \"we're\", 'weren', 'werent', \"weren't\", \"we've\", 'what', 'whatever', \"what's\", 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'with', 'within', 'without', 'wo', 'woh', 'wohi', 'won', 'wont', \"won't\", 'would', 'wouldn', 'wouldnt', \"wouldn't\", 'y', 'ya', 'yadi', 'yah', 'yaha', 'yahaan', 'yahan', 'yahi', 'yahin', 'ye', 'yeah', 'yeh', 'yehi', 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'yup']\n",
    "\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(lambda text: cleaning_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51db9e35",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2950457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "X = df['tweets']\n",
    "y = df['labels']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed107a4f",
   "metadata": {},
   "source": [
    "### Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9122e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  18879\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,1) - unigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,1), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "# Transforming the data using TD-IDF Vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e000b2c",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7dd05cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[[330   1   4  20   3   1   2]\n",
      " [ 45 165  11  19   0   1   1]\n",
      " [ 55  17 210  25   1   2   0]\n",
      " [ 60  11  15 253   1   2   4]\n",
      " [ 14   8   6   5  69   1   1]\n",
      " [ 31   3  11  20   1 172   1]\n",
      " [ 59   7   4  21   1   3 136]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.91      0.69       361\n",
      "           1       0.78      0.68      0.73       242\n",
      "           2       0.80      0.68      0.74       310\n",
      "           3       0.70      0.73      0.71       346\n",
      "           4       0.91      0.66      0.77       104\n",
      "           5       0.95      0.72      0.82       239\n",
      "           6       0.94      0.59      0.72       231\n",
      "\n",
      "    accuracy                           0.73      1833\n",
      "   macro avg       0.80      0.71      0.74      1833\n",
      "weighted avg       0.77      0.73      0.73      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[[329   0   4  21   3   2   2]\n",
      " [ 47 161  15  18   0   0   1]\n",
      " [ 52  16 219  21   0   2   0]\n",
      " [ 64   6  21 247   0   5   3]\n",
      " [ 15   7   9   3  68   0   2]\n",
      " [ 34   3  10  16   1 174   1]\n",
      " [ 66   5   3  15   2   4 136]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.91      0.68       361\n",
      "           1       0.81      0.67      0.73       242\n",
      "           2       0.78      0.71      0.74       310\n",
      "           3       0.72      0.71      0.72       346\n",
      "           4       0.92      0.65      0.76       104\n",
      "           5       0.93      0.73      0.82       239\n",
      "           6       0.94      0.59      0.72       231\n",
      "\n",
      "    accuracy                           0.73      1833\n",
      "   macro avg       0.81      0.71      0.74      1833\n",
      "weighted avg       0.77      0.73      0.73      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "RandomForestClassifier()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[[303   7   5  18   3   1  24]\n",
      " [ 25 176  17   8   0   1  15]\n",
      " [ 38  22 223  14   1   3   9]\n",
      " [ 51  22  19 240   0   4  10]\n",
      " [  7   6  12   2  73   0   4]\n",
      " [ 23  10  12   9   1 180   4]\n",
      " [ 41   6  10   5   1   2 166]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.84      0.71       361\n",
      "           1       0.71      0.73      0.72       242\n",
      "           2       0.75      0.72      0.73       310\n",
      "           3       0.81      0.69      0.75       346\n",
      "           4       0.92      0.70      0.80       104\n",
      "           5       0.94      0.75      0.84       239\n",
      "           6       0.72      0.72      0.72       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.78      0.74      0.75      1833\n",
      "weighted avg       0.76      0.74      0.75      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "Wall time: 8min 32s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.728314</td>\n",
       "      <td>{'C': 1, 'gamma': 1, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistics_regression</td>\n",
       "      <td>0.727769</td>\n",
       "      <td>{'C': 1, 'solver': 'lbfgs'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.742499</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 80}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  best_score  \\\n",
       "0                   SVC    0.728314   \n",
       "1  logistics_regression    0.727769   \n",
       "2         random_forest    0.742499   \n",
       "\n",
       "                                                    best_params  \n",
       "0                      {'C': 1, 'gamma': 1, 'kernel': 'linear'}  \n",
       "1                                   {'C': 1, 'solver': 'lbfgs'}  \n",
       "2  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 80}  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# implemented GridSearchCV for four models using a loop and a previously created dictionary\n",
    "# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "    print(mp['model'])\n",
    "    print('\\nFitting...')\n",
    "    best_model = clf.fit(X_train, y_train)                      # Training the model\n",
    "    clf_pred = best_model.predict(X_test)                       # Predicting the results\n",
    "    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n",
    "    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n",
    "    scores.append({                                             # Appending results to 'scores' list\n",
    "        'model' : model_name,\n",
    "        'best_score' : best_model.score(X_test, y_test),\n",
    "        'best_params' : clf.best_params_\n",
    "    })\n",
    "    print('\\nScore is appended.\\n')\n",
    "    \n",
    "# Creating data frame with model, best scores and best params:\n",
    "res = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56e494",
   "metadata": {},
   "source": [
    "# 3. Models after removing repeating characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5f5b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Code Mixed Hindi-English tweets.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de8920",
   "metadata": {},
   "source": [
    "### Removing repeating characteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03c6a3d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433466694110834689</td>\n",
       "      <td>[aaj, ka, khel, khatam, hone, k, baad, england, cricket, k, youtube, channel, par, mil, jayegi]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433466691330076675</td>\n",
       "      <td>[purana, manjan, bech, rha, hai]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1433463609858805764</td>\n",
       "      <td>[tumhare, disappointed, se, kuch, ni, hoga]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1433461022900117506</td>\n",
       "      <td>[inse, bas, tiktok, banva, lo, batting, ni, hoti, isse, cricket]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433460862728032257</td>\n",
       "      <td>[bhai, cricket, par, tweets, mat, kara, karo, please, jab, bhi, karte, ho, lanka, lag, jaati, hai, this, makes, me, angry]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>847143219754389504</td>\n",
       "      <td>[gujarati, fraud, kyu, hotey, hai, ?, pnbscam]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9161</th>\n",
       "      <td>909629251409911810</td>\n",
       "      <td>[saala, idhar, 7, lakh, k, car, loan, k, liye, bank, choos, leti, hai, aur, waha, log, crores, ka, fraud, karte, hai, bada, socho, niravmodi, pnbfraud]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9162</th>\n",
       "      <td>909419169644449793</td>\n",
       "      <td>[mitron, nirav, modi, ji, ka, relation, india, k, kaun, se, rich, industrialist, parivaar, se, hai, ?, batao, zara, ?, pnbfraud]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9163</th>\n",
       "      <td>909327295629877248</td>\n",
       "      <td>[or, yahn, ek, month, k, education, loan, ki, emi, pay, ni, karo, to, call, aa, jata, hai, pnbfraud, so, sad]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9164</th>\n",
       "      <td>909322844282089472</td>\n",
       "      <td>[jis, bank, me, 500, rupye, niklwane, k, liye, ghnto, khda, rehna, pdta, hai, unhone, aaj, btaya, k, $, 7, bn, koi, dokhe, se, idhr, udhr, kr, gya, pnb, fraud, this, makes, me, angry]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9165 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id  \\\n",
       "0     1433466694110834689   \n",
       "1     1433466691330076675   \n",
       "2     1433463609858805764   \n",
       "3     1433461022900117506   \n",
       "4     1433460862728032257   \n",
       "...                   ...   \n",
       "9160   847143219754389504   \n",
       "9161   909629251409911810   \n",
       "9162   909419169644449793   \n",
       "9163   909327295629877248   \n",
       "9164   909322844282089472   \n",
       "\n",
       "                                                                                                                                                                                       tweets  \\\n",
       "0                                                                                             [aaj, ka, khel, khatam, hone, k, baad, england, cricket, k, youtube, channel, par, mil, jayegi]   \n",
       "1                                                                                                                                                            [purana, manjan, bech, rha, hai]   \n",
       "2                                                                                                                                                 [tumhare, disappointed, se, kuch, ni, hoga]   \n",
       "3                                                                                                                            [inse, bas, tiktok, banva, lo, batting, ni, hoti, isse, cricket]   \n",
       "4                                                                  [bhai, cricket, par, tweets, mat, kara, karo, please, jab, bhi, karte, ho, lanka, lag, jaati, hai, this, makes, me, angry]   \n",
       "...                                                                                                                                                                                       ...   \n",
       "9160                                                                                                                                           [gujarati, fraud, kyu, hotey, hai, ?, pnbscam]   \n",
       "9161                                  [saala, idhar, 7, lakh, k, car, loan, k, liye, bank, choos, leti, hai, aur, waha, log, crores, ka, fraud, karte, hai, bada, socho, niravmodi, pnbfraud]   \n",
       "9162                                                         [mitron, nirav, modi, ji, ka, relation, india, k, kaun, se, rich, industrialist, parivaar, se, hai, ?, batao, zara, ?, pnbfraud]   \n",
       "9163                                                                            [or, yahn, ek, month, k, education, loan, ki, emi, pay, ni, karo, to, call, aa, jata, hai, pnbfraud, so, sad]   \n",
       "9164  [jis, bank, me, 500, rupye, niklwane, k, liye, ghnto, khda, rehna, pdta, hai, unhone, aaj, btaya, k, $, 7, bn, koi, dokhe, se, idhr, udhr, kr, gya, pnb, fraud, this, makes, me, angry]   \n",
       "\n",
       "      labels  \n",
       "0          0  \n",
       "1          5  \n",
       "2          0  \n",
       "3          3  \n",
       "4          3  \n",
       "...      ...  \n",
       "9160       3  \n",
       "9161       3  \n",
       "9162       3  \n",
       "9163       2  \n",
       "9164       3  \n",
       "\n",
       "[9165 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = (word_tokenize(i) for i in df.tweets)\n",
    "df['tweets'] = df['tweets'].apply(nltk.word_tokenize)\n",
    "#df['tweets'] = df['tweets'].astype(str)\n",
    "\n",
    "pattern = re.compile(r'(.)\\1*')\n",
    "\n",
    "def reduce_sequence_word(word):\n",
    "    return ''.join([match.group()[:2] if len(match.group()) > 2 else match.group() for match in pattern.finditer(word)])\n",
    "\n",
    "def reduce_sequence_tweet(tweet):\n",
    "    return [reduce_sequence_word(word) for word in tweet]\n",
    "\n",
    "df.tweets = df.tweets.apply(lambda tweet: reduce_sequence_tweet(tweet))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0ec7a",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a2e31e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "X = df['tweets'].astype(str)\n",
    "y = df['labels'].astype(str)\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d5ba08",
   "metadata": {},
   "source": [
    "### Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "609d73dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  19387\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,1) - unigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,1), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "# Transforming the data using TD-IDF Vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97621b32",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6e8e6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[[321   2   6  20   3   1   8]\n",
      " [ 45 170  12  14   0   1   0]\n",
      " [ 52  11 219  25   0   1   2]\n",
      " [ 59   5  12 259   0   7   4]\n",
      " [ 13   3   6   6  71   3   2]\n",
      " [ 35   3   4  21   1 175   0]\n",
      " [ 48   2   4  14   1   1 161]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.89      0.69       361\n",
      "           1       0.87      0.70      0.78       242\n",
      "           2       0.83      0.71      0.76       310\n",
      "           3       0.72      0.75      0.73       346\n",
      "           4       0.93      0.68      0.79       104\n",
      "           5       0.93      0.73      0.82       239\n",
      "           6       0.91      0.70      0.79       231\n",
      "\n",
      "    accuracy                           0.75      1833\n",
      "   macro avg       0.82      0.74      0.77      1833\n",
      "weighted avg       0.79      0.75      0.76      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[[322   1   9  18   2   1   8]\n",
      " [ 47 163  18  10   0   2   2]\n",
      " [ 57  13 214  20   0   3   3]\n",
      " [ 61   7  17 249   0  10   2]\n",
      " [ 19   2   6   3  70   2   2]\n",
      " [ 39   2   6  16   1 175   0]\n",
      " [ 55   4   3  14   0   3 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.89      0.67       361\n",
      "           1       0.85      0.67      0.75       242\n",
      "           2       0.78      0.69      0.73       310\n",
      "           3       0.75      0.72      0.74       346\n",
      "           4       0.96      0.67      0.79       104\n",
      "           5       0.89      0.73      0.80       239\n",
      "           6       0.90      0.66      0.76       231\n",
      "\n",
      "    accuracy                           0.73      1833\n",
      "   macro avg       0.81      0.72      0.75      1833\n",
      "weighted avg       0.78      0.73      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "RandomForestClassifier()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[[328   8   7   6   3   0   9]\n",
      " [ 45 177  12   4   0   1   3]\n",
      " [ 71  10 211  10   1   2   5]\n",
      " [ 88   9  11 227   1   5   5]\n",
      " [ 20   2   6   0  74   1   1]\n",
      " [ 41   7   4  15   1 171   0]\n",
      " [ 46   9   3   6   1   2 164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.91      0.66       361\n",
      "           1       0.80      0.73      0.76       242\n",
      "           2       0.83      0.68      0.75       310\n",
      "           3       0.85      0.66      0.74       346\n",
      "           4       0.91      0.71      0.80       104\n",
      "           5       0.94      0.72      0.81       239\n",
      "           6       0.88      0.71      0.78       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.82      0.73      0.76      1833\n",
      "weighted avg       0.79      0.74      0.75      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "Wall time: 9min 26s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.750682</td>\n",
       "      <td>{'C': 1, 'gamma': 1, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistics_regression</td>\n",
       "      <td>0.733770</td>\n",
       "      <td>{'C': 1, 'solver': 'lbfgs'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.737589</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 80}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  best_score  \\\n",
       "0                   SVC    0.750682   \n",
       "1  logistics_regression    0.733770   \n",
       "2         random_forest    0.737589   \n",
       "\n",
       "                                                    best_params  \n",
       "0                      {'C': 1, 'gamma': 1, 'kernel': 'linear'}  \n",
       "1                                   {'C': 1, 'solver': 'lbfgs'}  \n",
       "2  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 80}  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# implemented GridSearchCV for four models using a loop and a previously created dictionary\n",
    "# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "    print(mp['model'])\n",
    "    print('\\nFitting...')\n",
    "    best_model = clf.fit(X_train, y_train)                      # Training the model\n",
    "    clf_pred = best_model.predict(X_test)                       # Predicting the results\n",
    "    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n",
    "    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n",
    "    scores.append({                                             # Appending results to 'scores' list\n",
    "        'model' : model_name,\n",
    "        'best_score' : best_model.score(X_test, y_test),\n",
    "        'best_params' : clf.best_params_\n",
    "    })\n",
    "    print('\\nScore is appended.\\n')\n",
    "    \n",
    "# Creating data frame with model, best scores and best params:\n",
    "res = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a2438a",
   "metadata": {},
   "source": [
    "# 4. Models after removing Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff039581",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Code Mixed Hindi-English tweets.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0728240",
   "metadata": {},
   "source": [
    "### Removing Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8285cc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2df85f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations_list = string.punctuation\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(lambda x: cleaning_punctuations(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe75b7",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd0bc538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "X = df['tweets']\n",
    "y = df['labels']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c09df",
   "metadata": {},
   "source": [
    "### Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c5b62ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  19523\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,1) - unigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,1), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "# Transforming the data using TD-IDF Vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967cfbc7",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c71bb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[[323   2   5  19   3   1   8]\n",
      " [ 43 172  13  13   0   1   0]\n",
      " [ 54  11 218  24   0   1   2]\n",
      " [ 58   5  14 258   0   7   4]\n",
      " [ 14   3   6   6  70   3   2]\n",
      " [ 35   3   4  21   1 175   0]\n",
      " [ 45   4   4  14   2   1 161]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.89      0.69       361\n",
      "           1       0.86      0.71      0.78       242\n",
      "           2       0.83      0.70      0.76       310\n",
      "           3       0.73      0.75      0.74       346\n",
      "           4       0.92      0.67      0.78       104\n",
      "           5       0.93      0.73      0.82       239\n",
      "           6       0.91      0.70      0.79       231\n",
      "\n",
      "    accuracy                           0.75      1833\n",
      "   macro avg       0.82      0.74      0.76      1833\n",
      "weighted avg       0.79      0.75      0.76      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[[323   1   9  17   2   1   8]\n",
      " [ 47 162  19  10   0   2   2]\n",
      " [ 58  13 213  20   0   3   3]\n",
      " [ 59   6  17 250   0  11   3]\n",
      " [ 19   2   6   3  70   2   2]\n",
      " [ 39   2   7  16   1 174   0]\n",
      " [ 54   4   3  13   1   3 153]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.89      0.67       361\n",
      "           1       0.85      0.67      0.75       242\n",
      "           2       0.78      0.69      0.73       310\n",
      "           3       0.76      0.72      0.74       346\n",
      "           4       0.95      0.67      0.79       104\n",
      "           5       0.89      0.73      0.80       239\n",
      "           6       0.89      0.66      0.76       231\n",
      "\n",
      "    accuracy                           0.73      1833\n",
      "   macro avg       0.81      0.72      0.75      1833\n",
      "weighted avg       0.78      0.73      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "RandomForestClassifier()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[[324   5   6  11   2   1  12]\n",
      " [ 46 178  13   2   0   0   3]\n",
      " [ 72  14 209   9   1   0   5]\n",
      " [ 86  14  16 224   0   3   3]\n",
      " [ 21   1   6   0  73   2   1]\n",
      " [ 41   6   5  10   3 173   1]\n",
      " [ 48  11   4   5   0   0 163]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.90      0.65       361\n",
      "           1       0.78      0.74      0.76       242\n",
      "           2       0.81      0.67      0.73       310\n",
      "           3       0.86      0.65      0.74       346\n",
      "           4       0.92      0.70      0.80       104\n",
      "           5       0.97      0.72      0.83       239\n",
      "           6       0.87      0.71      0.78       231\n",
      "\n",
      "    accuracy                           0.73      1833\n",
      "   macro avg       0.82      0.73      0.75      1833\n",
      "weighted avg       0.79      0.73      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "Wall time: 11min 13s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.751227</td>\n",
       "      <td>{'C': 1, 'gamma': 1, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistics_regression</td>\n",
       "      <td>0.733770</td>\n",
       "      <td>{'C': 1, 'solver': 'lbfgs'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.733224</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 85}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  best_score  \\\n",
       "0                   SVC    0.751227   \n",
       "1  logistics_regression    0.733770   \n",
       "2         random_forest    0.733224   \n",
       "\n",
       "                                                    best_params  \n",
       "0                      {'C': 1, 'gamma': 1, 'kernel': 'linear'}  \n",
       "1                                   {'C': 1, 'solver': 'lbfgs'}  \n",
       "2  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 85}  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# implemented GridSearchCV for four models using a loop and a previously created dictionary\n",
    "# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n",
    "\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "    print(mp['model'])\n",
    "    print('\\nFitting...')\n",
    "    best_model = clf.fit(X_train, y_train)                      # Training the model\n",
    "    clf_pred = best_model.predict(X_test)                       # Predicting the results\n",
    "    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n",
    "    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n",
    "    scores.append({                                             # Appending results to 'scores' list\n",
    "        'model' : model_name,\n",
    "        'best_score' : best_model.score(X_test, y_test),\n",
    "        'best_params' : clf.best_params_\n",
    "    })\n",
    "    print('\\nScore is appended.\\n')\n",
    "    \n",
    "# Creating data frame with model, best scores and best params:\n",
    "res = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b69532",
   "metadata": {},
   "source": [
    "# 5. Models after removing numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11a0453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Code Mixed Hindi-English tweets.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc2ec9",
   "metadata": {},
   "source": [
    "### Removing numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a102ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(text):\n",
    "    return re.sub('[0-9]+', '', text)\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(lambda text: cleaning_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e13d25",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7dd763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "X = df['tweets']\n",
    "y = df['labels']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7e164",
   "metadata": {},
   "source": [
    "### Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6251b00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  19143\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,1) - unigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,1), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "# Transforming the data using TD-IDF Vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7152ffe",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eece2830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[[322   2   6  19   3   1   8]\n",
      " [ 45 171  11  14   0   1   0]\n",
      " [ 54  11 218  24   0   1   2]\n",
      " [ 58   5  13 259   0   7   4]\n",
      " [ 13   3   6   6  71   3   2]\n",
      " [ 34   3   4  20   1 176   1]\n",
      " [ 48   4   3  12   2   2 160]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.89      0.69       361\n",
      "           1       0.86      0.71      0.78       242\n",
      "           2       0.84      0.70      0.76       310\n",
      "           3       0.73      0.75      0.74       346\n",
      "           4       0.92      0.68      0.78       104\n",
      "           5       0.92      0.74      0.82       239\n",
      "           6       0.90      0.69      0.78       231\n",
      "\n",
      "    accuracy                           0.75      1833\n",
      "   macro avg       0.82      0.74      0.77      1833\n",
      "weighted avg       0.79      0.75      0.76      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[[323   1   9  17   2   1   8]\n",
      " [ 47 163  17  11   0   2   2]\n",
      " [ 57  13 214  20   0   3   3]\n",
      " [ 62   5  18 249   0  10   2]\n",
      " [ 19   2   6   3  70   2   2]\n",
      " [ 39   3   6  15   1 175   0]\n",
      " [ 56   4   3  14   1   3 150]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.89      0.67       361\n",
      "           1       0.85      0.67      0.75       242\n",
      "           2       0.78      0.69      0.73       310\n",
      "           3       0.76      0.72      0.74       346\n",
      "           4       0.95      0.67      0.79       104\n",
      "           5       0.89      0.73      0.80       239\n",
      "           6       0.90      0.65      0.75       231\n",
      "\n",
      "    accuracy                           0.73      1833\n",
      "   macro avg       0.81      0.72      0.75      1833\n",
      "weighted avg       0.78      0.73      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "RandomForestClassifier()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[[323   7   8   8   3   0  12]\n",
      " [ 52 171  10   3   0   0   6]\n",
      " [ 78  12 208   8   1   0   3]\n",
      " [ 89  10  14 224   0   6   3]\n",
      " [ 16   3   4   0  77   1   3]\n",
      " [ 44   7   6  10   2 170   0]\n",
      " [ 49   9   4   3   1   1 164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.89      0.64       361\n",
      "           1       0.78      0.71      0.74       242\n",
      "           2       0.82      0.67      0.74       310\n",
      "           3       0.88      0.65      0.74       346\n",
      "           4       0.92      0.74      0.82       104\n",
      "           5       0.96      0.71      0.82       239\n",
      "           6       0.86      0.71      0.78       231\n",
      "\n",
      "    accuracy                           0.73      1833\n",
      "   macro avg       0.81      0.73      0.75      1833\n",
      "weighted avg       0.79      0.73      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "Wall time: 10min 32s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.751227</td>\n",
       "      <td>{'C': 1, 'gamma': 1, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistics_regression</td>\n",
       "      <td>0.733224</td>\n",
       "      <td>{'C': 1, 'solver': 'lbfgs'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.729405</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 80}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  best_score  \\\n",
       "0                   SVC    0.751227   \n",
       "1  logistics_regression    0.733224   \n",
       "2         random_forest    0.729405   \n",
       "\n",
       "                                                    best_params  \n",
       "0                      {'C': 1, 'gamma': 1, 'kernel': 'linear'}  \n",
       "1                                   {'C': 1, 'solver': 'lbfgs'}  \n",
       "2  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 80}  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# implemented GridSearchCV for four models using a loop and a previously created dictionary\n",
    "# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "    print(mp['model'])\n",
    "    print('\\nFitting...')\n",
    "    best_model = clf.fit(X_train, y_train)                      # Training the model\n",
    "    clf_pred = best_model.predict(X_test)                       # Predicting the results\n",
    "    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n",
    "    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n",
    "    scores.append({                                             # Appending results to 'scores' list\n",
    "        'model' : model_name,\n",
    "        'best_score' : best_model.score(X_test, y_test),\n",
    "        'best_params' : clf.best_params_\n",
    "    })\n",
    "    print('\\nScore is appended.\\n')\n",
    "    \n",
    "# Creating data frame with model, best scores and best params:\n",
    "res = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050df75",
   "metadata": {},
   "source": [
    "# 6. Models after removing all the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8021ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Code Mixed Hindi-English tweets.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f0db7",
   "metadata": {},
   "source": [
    "### Removing Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3c691c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18fa6132",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations_list = string.punctuation\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(lambda x: cleaning_punctuations(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0bb5a",
   "metadata": {},
   "source": [
    "### Removing Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62338c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hinglish stopwords which contains 1036 words from both English and Hindi languages\n",
    "# Source: https://github.com/TrigonaMinima/HinglishNLP/blob/master/data/assets/stop_hinglish\n",
    "\n",
    "stopwordlist = ['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab', 'abbe', 'abbey', 'abe', 'abhi', 'able', 'about', 'above', 'accha', 'according', 'accordingly', 'acha', 'achcha', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'agar', 'ain', 'aint', \"ain't\", 'aisa', 'aise', 'aisi', 'alag', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'andar', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'ap', 'apan', 'apart', 'apna', 'apnaa', 'apne', 'apni', 'appear', 'are', 'aren', 'arent', \"aren't\", 'around', 'arre', 'as', 'aside', 'ask', 'asking', 'at', 'aur', 'avum', 'aya', 'aye', 'baad', 'baar', 'bad', 'bahut', 'bana', 'banae', 'banai', 'banao', 'banaya', 'banaye', 'banayi', 'banda', 'bande', 'bandi', 'bane', 'bani', 'bas', 'bata', 'batao', 'bc', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'bhai', 'bheetar', 'bhi', 'bhitar', 'bht', 'bilkul', 'bohot', 'bol', 'bola', 'bole', 'boli', 'bolo', 'bolta', 'bolte', 'bolti', 'both', 'brief', 'bro', 'btw', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain', 'certainly', 'chahiye', 'chaiye', 'chal', 'chalega', 'chhaiye', 'clearly', \"c'mon\", 'com', 'come', 'comes', 'could', 'couldn', 'couldnt', \"couldn't\", 'd', 'de', 'dede', 'dega', 'degi', 'dekh', 'dekha', 'dekhe', 'dekhi', 'dekho', 'denge', 'dhang', 'di', 'did', 'didn', 'didnt', \"didn't\", 'dijiye', 'diya', 'diyaa', 'diye', 'diyo', 'do', 'does', 'doesn', 'doesnt', \"doesn't\", 'doing', 'done', 'dono', 'dont', \"don't\", 'doosra', 'doosre', 'down', 'downwards', 'dude', 'dunga', 'dungi', 'during', 'dusra', 'dusre', 'dusri', 'dvaara', 'dvara', 'dwaara', 'dwara', 'each', 'edu', 'eg', 'eight', 'either', 'ek', 'else', 'elsewhere', 'enough', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'fir', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forth', 'four', 'from', 'further', 'furthermore', 'gaya', 'gaye', 'gayi', 'get', 'gets', 'getting', 'ghar', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'good', 'got', 'gotten', 'greetings', 'haan', 'had', 'hadd', 'hadn', 'hadnt', \"hadn't\", 'hai', 'hain', 'hamara', 'hamare', 'hamari', 'hamne', 'han', 'happens', 'har', 'hardly', 'has', 'hasn', 'hasnt', \"hasn't\", 'have', 'haven', 'havent', \"haven't\", 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', \"here's\", 'hereupon', 'hers', 'herself', \"he's\", 'hi', 'him', 'himself', 'his', 'hither', 'hm', 'hmm', 'ho', 'hoga', 'hoge', 'hogi', 'hona', 'honaa', 'hone', 'honge', 'hongi', 'honi', 'hopefully', 'hota', 'hotaa', 'hote', 'hoti', 'how', 'howbeit', 'however', 'hoyenge', 'hoyengi', 'hu', 'hua', 'hue', 'huh', 'hui', 'hum', 'humein', 'humne', 'hun', 'huye', 'huyi', 'i', \"i'd\", 'idk', 'ie', 'if', \"i'll\", \"i'm\", 'imo', 'in', 'inasmuch', 'inc', 'inhe', 'inhi', 'inho', 'inka', 'inkaa', 'inke', 'inki', 'inn', 'inner', 'inse', 'insofar', 'into', 'inward', 'is', 'ise', 'isi', 'iska', 'iskaa', 'iske', 'iski', 'isme', 'isn', 'isne', 'isnt', \"isn't\", 'iss', 'isse', 'issi', 'isski', 'it', \"it'd\", \"it'll\", 'itna', 'itne', 'itni', 'itno', 'its', \"it's\", 'itself', 'ityaadi', 'ityadi', \"i've\", 'ja', 'jaa', 'jab', 'jabh', 'jaha', 'jahaan', 'jahan', 'jaisa', 'jaise', 'jaisi', 'jata', 'jayega', 'jidhar', 'jin', 'jinhe', 'jinhi', 'jinho', 'jinhone', 'jinka', 'jinke', 'jinki', 'jinn', 'jis', 'jise', 'jiska', 'jiske', 'jiski', 'jisme', 'jiss', 'jisse', 'jitna', 'jitne', 'jitni', 'jo', 'just', 'jyaada', 'jyada', 'k', 'ka', 'kaafi', 'kab', 'kabhi', 'kafi', 'kaha', 'kahaa', 'kahaan', 'kahan', 'kahi', 'kahin', 'kahte', 'kaisa', 'kaise', 'kaisi', 'kal', 'kam', 'kar', 'kara', 'kare', 'karega', 'karegi', 'karen', 'karenge', 'kari', 'karke', 'karna', 'karne', 'karni', 'karo', 'karta', 'karte', 'karti', 'karu', 'karun', 'karunga', 'karungi', 'kaun', 'kaunsa', 'kayi', 'kch', 'ke', 'keep', 'keeps', 'keh', 'kehte', 'kept', 'khud', 'ki', 'kin', 'kine', 'kinhe', 'kinho', 'kinka', 'kinke', 'kinki', 'kinko', 'kinn', 'kino', 'kis', 'kise', 'kisi', 'kiska', 'kiske', 'kiski', 'kisko', 'kisliye', 'kisne', 'kitna', 'kitne', 'kitni', 'kitno', 'kiya', 'kiye', 'know', 'known', 'knows', 'ko', 'koi', 'kon', 'konsa', 'koyi', 'krna', 'krne', 'kuch', 'kuchch', 'kuchh', 'kul', 'kull', 'kya', 'kyaa', 'kyu', 'kyuki', 'kyun', 'kyunki', 'lagta', 'lagte', 'lagti', 'last', 'lately', 'later', 'le', 'least', 'lekar', 'lekin', 'less', 'lest', 'let', \"let's\", 'li', 'like', 'liked', 'likely', 'little', 'liya', 'liye', 'll', 'lo', 'log', 'logon', 'lol', 'look', 'looking', 'looks', 'ltd', 'lunga', 'm', 'maan', 'maana', 'maane', 'maani', 'maano', 'magar', 'mai', 'main', 'maine', 'mainly', 'mana', 'mane', 'mani', 'mano', 'many', 'mat', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'mein', 'mera', 'mere', 'merely', 'meri', 'might', 'mightn', 'mightnt', \"mightn't\", 'mil', 'mjhe', 'more', 'moreover', 'most', 'mostly', 'much', 'mujhe', 'must', 'mustn', 'mustnt', \"mustn't\", 'my', 'myself', 'na', 'naa', 'naah', 'nahi', 'nahin', 'nai', 'name', 'namely', 'nd', 'ne', 'near', 'nearly', 'necessary', 'neeche', 'need', 'needn', 'neednt', \"needn't\", 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nhi', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nope', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'par', 'pata', 'pe', 'pehla', 'pehle', 'pehli', 'people', 'per', 'perhaps', 'phla', 'phle', 'phli', 'placed', 'please', 'plus', 'poora', 'poori', 'provides', 'pura', 'puri', 'q', 'que', 'quite', 'raha', 'rahaa', 'rahe', 'rahi', 'rakh', 'rakha', 'rakhe', 'rakhen', 'rakhi', 'rakho', 'rather', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'rehte', 'rha', 'rhaa', 'rhe', 'rhi', 'ri', 'right', 's', 'sa', 'saara', 'saare', 'saath', 'sab', 'sabhi', 'sabse', 'sahi', 'said', 'sakta', 'saktaa', 'sakte', 'sakti', 'same', 'sang', 'sara', 'sath', 'saw', 'say', 'saying', 'says', 'se', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'shan', 'shant', \"shan't\", 'she', \"she's\", 'should', 'shouldn', 'shouldnt', \"shouldn't\", \"should've\", 'si', 'since', 'six', 'so', 'soch', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'still', 'sub', 'such', 'sup', 'sure', 't', 'tab', 'tabh', 'tak', 'take', 'taken', 'tarah', 'teen', 'teeno', 'teesra', 'teesre', 'teesri', 'tell', 'tends', 'tera', 'tere', 'teri', 'th', 'tha', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that's\", 'the', 'theek', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', \"there's\", 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'thi', 'thik', 'thing', 'think', 'thinking', 'third', 'this', 'tho', 'thoda', 'thodi', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'three', 'through', 'throughout', 'thru', 'thus', 'tjhe', 'to', 'together', 'toh', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'true', 'truly', 'try', 'trying', 'tu', 'tujhe', 'tum', 'tumhara', 'tumhare', 'tumhari', 'tune', 'twice', 'two', 'um', 'umm', 'un', 'under', 'unhe', 'unhi', 'unho', 'unhone', 'unka', 'unkaa', 'unke', 'unki', 'unko', 'unless', 'unlikely', 'unn', 'unse', 'until', 'unto', 'up', 'upar', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'usi', 'using', 'uska', 'uske', 'usne', 'uss', 'usse', 'ussi', 'usually', 'vaala', 'vaale', 'vaali', 'vahaan', 'vahan', 'vahi', 'vahin', 'vaisa', 'vaise', 'vaisi', 'vala', 'vale', 'vali', 'various', 've', 'very', 'via', 'viz', 'vo', 'waala', 'waale', 'waali', 'wagaira', 'wagairah', 'wagerah', 'waha', 'wahaan', 'wahan', 'wahi', 'wahin', 'waisa', 'waise', 'waisi', 'wala', 'wale', 'wali', 'want', 'wants', 'was', 'wasn', 'wasnt', \"wasn't\", 'way', 'we', \"we'd\", 'well', \"we'll\", 'went', 'were', \"we're\", 'weren', 'werent', \"weren't\", \"we've\", 'what', 'whatever', \"what's\", 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'with', 'within', 'without', 'wo', 'woh', 'wohi', 'won', 'wont', \"won't\", 'would', 'wouldn', 'wouldnt', \"wouldn't\", 'y', 'ya', 'yadi', 'yah', 'yaha', 'yahaan', 'yahan', 'yahi', 'yahin', 'ye', 'yeah', 'yeh', 'yehi', 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'yup']\n",
    "\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(lambda text: cleaning_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95f4c3",
   "metadata": {},
   "source": [
    "### Removing Numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d295ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(text):\n",
    "    return re.sub('[0-9]+', '', text)\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(lambda text: cleaning_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46860f1b",
   "metadata": {},
   "source": [
    "### Removing repeating characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17cac212",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = (word_tokenize(i) for i in df.tweets)\n",
    "df['tweets'] = df['tweets'].apply(nltk.word_tokenize)\n",
    "#df['tweets'] = df['tweets'].astype(str)\n",
    "\n",
    "pattern = re.compile(r'(.)\\1*')\n",
    "\n",
    "def reduce_sequence_word(word):\n",
    "    return ''.join([match.group()[:2] if len(match.group()) > 2 else match.group() for match in pattern.finditer(word)])\n",
    "\n",
    "def reduce_sequence_tweet(tweet):\n",
    "    return [reduce_sequence_word(word) for word in tweet]\n",
    "\n",
    "df.tweets = df.tweets.apply(lambda tweet: reduce_sequence_tweet(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68f356",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5010bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "X = df['tweets'].astype(str)\n",
    "y = df['labels'].astype(str)\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3062ec",
   "metadata": {},
   "source": [
    "### Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e8fbcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  18436\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,1) - unigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,1), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "# Transforming the data using TD-IDF Vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a42724",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6dad5e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[[328   1   4  22   3   1   2]\n",
      " [ 47 167   9  17   0   1   1]\n",
      " [ 55  17 212  24   1   1   0]\n",
      " [ 59  13  14 251   1   3   5]\n",
      " [ 14   7   7   5  69   1   1]\n",
      " [ 29   3  10  21   1 174   1]\n",
      " [ 59   8   4  21   1   3 135]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.91      0.69       361\n",
      "           1       0.77      0.69      0.73       242\n",
      "           2       0.82      0.68      0.74       310\n",
      "           3       0.70      0.73      0.71       346\n",
      "           4       0.91      0.66      0.77       104\n",
      "           5       0.95      0.73      0.82       239\n",
      "           6       0.93      0.58      0.72       231\n",
      "\n",
      "    accuracy                           0.73      1833\n",
      "   macro avg       0.80      0.71      0.74      1833\n",
      "weighted avg       0.77      0.73      0.73      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[[333   0   4  19   3   1   1]\n",
      " [ 50 161  14  16   0   0   1]\n",
      " [ 55  12 220  21   0   2   0]\n",
      " [ 67   7  19 246   0   5   2]\n",
      " [ 15   5  10   3  69   0   2]\n",
      " [ 33   3   9  17   1 175   1]\n",
      " [ 69   5   4  14   1   1 137]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.92      0.68       361\n",
      "           1       0.83      0.67      0.74       242\n",
      "           2       0.79      0.71      0.75       310\n",
      "           3       0.73      0.71      0.72       346\n",
      "           4       0.93      0.66      0.78       104\n",
      "           5       0.95      0.73      0.83       239\n",
      "           6       0.95      0.59      0.73       231\n",
      "\n",
      "    accuracy                           0.73      1833\n",
      "   macro avg       0.82      0.71      0.75      1833\n",
      "weighted avg       0.78      0.73      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "RandomForestClassifier()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[[314   2   5  14   3   1  22]\n",
      " [ 23 176  14   9   0   1  19]\n",
      " [ 38  18 226  15   1   1  11]\n",
      " [ 55  19  21 236   2   5   8]\n",
      " [  7   6   8   3  74   1   5]\n",
      " [ 25   8   9  14   2 178   3]\n",
      " [ 42   6   8   6   2   1 166]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.87      0.73       361\n",
      "           1       0.75      0.73      0.74       242\n",
      "           2       0.78      0.73      0.75       310\n",
      "           3       0.79      0.68      0.73       346\n",
      "           4       0.88      0.71      0.79       104\n",
      "           5       0.95      0.74      0.83       239\n",
      "           6       0.71      0.72      0.71       231\n",
      "\n",
      "    accuracy                           0.75      1833\n",
      "   macro avg       0.78      0.74      0.76      1833\n",
      "weighted avg       0.77      0.75      0.75      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "Wall time: 7min 9s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.728860</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1, 'kernel': 'sigmoid'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistics_regression</td>\n",
       "      <td>0.731588</td>\n",
       "      <td>{'C': 1, 'solver': 'liblinear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.747409</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 95}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  best_score  \\\n",
       "0                   SVC    0.728860   \n",
       "1  logistics_regression    0.731588   \n",
       "2         random_forest    0.747409   \n",
       "\n",
       "                                                    best_params  \n",
       "0                  {'C': 10, 'gamma': 0.1, 'kernel': 'sigmoid'}  \n",
       "1                               {'C': 1, 'solver': 'liblinear'}  \n",
       "2  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 95}  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# implemented GridSearchCV for four models using a loop and a previously created dictionary\n",
    "# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "    print(mp['model'])\n",
    "    print('\\nFitting...')\n",
    "    best_model = clf.fit(X_train, y_train)                      # Training the model\n",
    "    clf_pred = best_model.predict(X_test)                       # Predicting the results\n",
    "    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n",
    "    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n",
    "    scores.append({                                             # Appending results to 'scores' list\n",
    "        'model' : model_name,\n",
    "        'best_score' : best_model.score(X_test, y_test),\n",
    "        'best_params' : clf.best_params_\n",
    "    })\n",
    "    print('\\nScore is appended.\\n')\n",
    "    \n",
    "# Creating data frame with model, best scores and best params:\n",
    "res = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb937290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
