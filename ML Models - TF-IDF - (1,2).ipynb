{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f0aa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sanket\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sanket\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries:\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ddcebc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaj ka khel khatam hone k baad england cricket k youtube channel par mil jayegi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>purana manjan bech rha hai</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tumhare disappointed se kuch ni hoga</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inse bas tiktok banva lo batting ni hoti isse cricket</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bhai cricket par tweets mat kara karo please jab bhi karte ho lanka lag jaati hai this makes me angry</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>gujarati fraud kyu hotey hai? pnbscam</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9161</th>\n",
       "      <td>saala idhar 7 lakh k car loan k liye bank chooos leti hai aur waha log crores ka fraud karte hai bada socho niravmodi pnbfraud</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9162</th>\n",
       "      <td>mitron nirav modi ji ka relation india k kaun se rich industrialist parivaar se hai ? batao zara ? pnbfraud</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9163</th>\n",
       "      <td>or yahn ek month k education loan ki emi pay ni karo to call aa jata hai pnbfraud  so sad</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9164</th>\n",
       "      <td>jis bank me 500 rupye niklwane k liye ghnto khda rehna pdta hai unhone aaj btaya k $7 bn koi dokhe se idhr udhr kr gya pnb fraud this makes me angry</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9165 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                     tweets  \\\n",
       "0                                                                           aaj ka khel khatam hone k baad england cricket k youtube channel par mil jayegi   \n",
       "1                                                                                                                                purana manjan bech rha hai   \n",
       "2                                                                                                                      tumhare disappointed se kuch ni hoga   \n",
       "3                                                                                                     inse bas tiktok banva lo batting ni hoti isse cricket   \n",
       "4                                                    bhai cricket par tweets mat kara karo please jab bhi karte ho lanka lag jaati hai this makes me angry    \n",
       "...                                                                                                                                                     ...   \n",
       "9160                                                                                                                  gujarati fraud kyu hotey hai? pnbscam   \n",
       "9161                         saala idhar 7 lakh k car loan k liye bank chooos leti hai aur waha log crores ka fraud karte hai bada socho niravmodi pnbfraud   \n",
       "9162                                            mitron nirav modi ji ka relation india k kaun se rich industrialist parivaar se hai ? batao zara ? pnbfraud   \n",
       "9163                                                             or yahn ek month k education loan ki emi pay ni karo to call aa jata hai pnbfraud  so sad    \n",
       "9164  jis bank me 500 rupye niklwane k liye ghnto khda rehna pdta hai unhone aaj btaya k $7 bn koi dokhe se idhr udhr kr gya pnb fraud this makes me angry    \n",
       "\n",
       "      labels  \n",
       "0          0  \n",
       "1          5  \n",
       "2          0  \n",
       "3          3  \n",
       "4          3  \n",
       "...      ...  \n",
       "9160       3  \n",
       "9161       3  \n",
       "9162       3  \n",
       "9163       2  \n",
       "9164       3  \n",
       "\n",
       "[9165 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Code Mixed Hindi-English tweets.xlsx')\n",
    "df.drop(['id'],axis=1, inplace=True)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None) # to display full length text of column.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df2e90",
   "metadata": {},
   "source": [
    "# Labels are as follows:\n",
    "0 - No emotions,\n",
    "1 - Happy,\n",
    "2 - Sad,\n",
    "3 - Angry,\n",
    "4 - Fear,\n",
    "5 - Disgust,\n",
    "6 - Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7d62f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (9165, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1892\n",
       "3    1763\n",
       "2    1529\n",
       "1    1226\n",
       "5    1147\n",
       "6    1049\n",
       "4     559\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset shape: \", df.shape)\n",
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1408b36",
   "metadata": {},
   "source": [
    "# 1. Models without Removing anything:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0883e775",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b9dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "X = df['tweets']\n",
    "y = df['labels']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feded1c5",
   "metadata": {},
   "source": [
    "### Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f16534d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  114324\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,2) - unigrams and bigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "# Transforming the data using TD-IDF Vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c42afa",
   "metadata": {},
   "source": [
    "### Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095c5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dictionary with four models with some parameters:\n",
    "model_params = {\n",
    "    \n",
    "    'SVC' :{\n",
    "        'model' : SVC(),\n",
    "        'params' : {\n",
    "            'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01], 'kernel': ['rbf','linear','poly','sigmoid']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'logistics_regression' :{\n",
    "        'model' : LogisticRegression(solver = 'lbfgs', multi_class = 'auto'),\n",
    "        'params' : {\n",
    "            'C' : [0.1, 1, 20, 40, 60, 80, 100], 'solver' : ['lbfgs', 'liblinear']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'random_forest' :{\n",
    "        'model' : RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            'n_estimators' : [80,85,90,95,100], \n",
    "            'max_depth':[20,30,None], 'criterion':['gini','entropy']\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3278e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[[298   3   9  26   5   7  13]\n",
      " [ 43 165  14  13   0   4   3]\n",
      " [ 43  10 227  20   0   7   3]\n",
      " [ 51   9  18 247   1  18   2]\n",
      " [ 12   3   4   6  74   2   3]\n",
      " [ 28   1   8  16   1 185   0]\n",
      " [ 41   8   5  13   1   3 160]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.83      0.68       361\n",
      "           1       0.83      0.68      0.75       242\n",
      "           2       0.80      0.73      0.76       310\n",
      "           3       0.72      0.71      0.72       346\n",
      "           4       0.90      0.71      0.80       104\n",
      "           5       0.82      0.77      0.80       239\n",
      "           6       0.87      0.69      0.77       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.79      0.73      0.75      1833\n",
      "weighted avg       0.76      0.74      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[[311   3   7  21   4   2  13]\n",
      " [ 42 165  13  13   0   4   5]\n",
      " [ 46  10 226  21   0   5   2]\n",
      " [ 57   4  18 245   1  18   3]\n",
      " [ 16   3   4   4  73   2   2]\n",
      " [ 29   2   7  17   1 183   0]\n",
      " [ 49   3   5  10   1   4 159]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.86      0.68       361\n",
      "           1       0.87      0.68      0.76       242\n",
      "           2       0.81      0.73      0.77       310\n",
      "           3       0.74      0.71      0.72       346\n",
      "           4       0.91      0.70      0.79       104\n",
      "           5       0.84      0.77      0.80       239\n",
      "           6       0.86      0.69      0.77       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.80      0.73      0.76      1833\n",
      "weighted avg       0.77      0.74      0.75      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "RandomForestClassifier()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[[333   7   3   3   3   0  12]\n",
      " [ 57 173   4   2   0   0   6]\n",
      " [ 78  15 201   6   1   0   9]\n",
      " [109   5   7 211   2   4   8]\n",
      " [ 23   1   3   0  76   0   1]\n",
      " [ 55   6   4   6   1 164   3]\n",
      " [ 49  10   2   4   0   0 166]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.92      0.63       361\n",
      "           1       0.80      0.71      0.75       242\n",
      "           2       0.90      0.65      0.75       310\n",
      "           3       0.91      0.61      0.73       346\n",
      "           4       0.92      0.73      0.81       104\n",
      "           5       0.98      0.69      0.81       239\n",
      "           6       0.81      0.72      0.76       231\n",
      "\n",
      "    accuracy                           0.72      1833\n",
      "   macro avg       0.83      0.72      0.75      1833\n",
      "weighted avg       0.80      0.72      0.73      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "Wall time: 19min 29s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.739771</td>\n",
       "      <td>{'C': 10, 'gamma': 1, 'kernel': 'sigmoid'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistics_regression</td>\n",
       "      <td>0.743044</td>\n",
       "      <td>{'C': 100, 'solver': 'lbfgs'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.722313</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 95}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  best_score  \\\n",
       "0                   SVC    0.739771   \n",
       "1  logistics_regression    0.743044   \n",
       "2         random_forest    0.722313   \n",
       "\n",
       "                                                    best_params  \n",
       "0                    {'C': 10, 'gamma': 1, 'kernel': 'sigmoid'}  \n",
       "1                                 {'C': 100, 'solver': 'lbfgs'}  \n",
       "2  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 95}  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# implemented GridSearchCV for four models using a loop and a previously created dictionary\n",
    "# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "    print(mp['model'])\n",
    "    print('\\nFitting...')\n",
    "    best_model = clf.fit(X_train, y_train)                      # Training the model\n",
    "    clf_pred = best_model.predict(X_test)                       # Predicting the results\n",
    "    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n",
    "    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n",
    "    scores.append({                                             # Appending results to 'scores' list\n",
    "        'model' : model_name,\n",
    "        'best_score' : best_model.score(X_test, y_test),\n",
    "        'best_params' : clf.best_params_\n",
    "    })\n",
    "    print('\\nScore is appended.\\n')\n",
    "    \n",
    "# Creating data frame with model, best scores and best params:\n",
    "res = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3249e64",
   "metadata": {},
   "source": [
    "# 2. Models after removing Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f5e2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Code Mixed Hindi-English tweets.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523143f1",
   "metadata": {},
   "source": [
    "### Removing Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3cd79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hinglish stopwords which contains 1036 words from both English and Hindi languages\n",
    "# Source: https://github.com/TrigonaMinima/HinglishNLP/blob/master/data/assets/stop_hinglish\n",
    "\n",
    "stopwordlist = ['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab', 'abbe', 'abbey', 'abe', 'abhi', 'able', 'about', 'above', 'accha', 'according', 'accordingly', 'acha', 'achcha', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'agar', 'ain', 'aint', \"ain't\", 'aisa', 'aise', 'aisi', 'alag', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'andar', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'ap', 'apan', 'apart', 'apna', 'apnaa', 'apne', 'apni', 'appear', 'are', 'aren', 'arent', \"aren't\", 'around', 'arre', 'as', 'aside', 'ask', 'asking', 'at', 'aur', 'avum', 'aya', 'aye', 'baad', 'baar', 'bad', 'bahut', 'bana', 'banae', 'banai', 'banao', 'banaya', 'banaye', 'banayi', 'banda', 'bande', 'bandi', 'bane', 'bani', 'bas', 'bata', 'batao', 'bc', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'bhai', 'bheetar', 'bhi', 'bhitar', 'bht', 'bilkul', 'bohot', 'bol', 'bola', 'bole', 'boli', 'bolo', 'bolta', 'bolte', 'bolti', 'both', 'brief', 'bro', 'btw', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain', 'certainly', 'chahiye', 'chaiye', 'chal', 'chalega', 'chhaiye', 'clearly', \"c'mon\", 'com', 'come', 'comes', 'could', 'couldn', 'couldnt', \"couldn't\", 'd', 'de', 'dede', 'dega', 'degi', 'dekh', 'dekha', 'dekhe', 'dekhi', 'dekho', 'denge', 'dhang', 'di', 'did', 'didn', 'didnt', \"didn't\", 'dijiye', 'diya', 'diyaa', 'diye', 'diyo', 'do', 'does', 'doesn', 'doesnt', \"doesn't\", 'doing', 'done', 'dono', 'dont', \"don't\", 'doosra', 'doosre', 'down', 'downwards', 'dude', 'dunga', 'dungi', 'during', 'dusra', 'dusre', 'dusri', 'dvaara', 'dvara', 'dwaara', 'dwara', 'each', 'edu', 'eg', 'eight', 'either', 'ek', 'else', 'elsewhere', 'enough', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'fir', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forth', 'four', 'from', 'further', 'furthermore', 'gaya', 'gaye', 'gayi', 'get', 'gets', 'getting', 'ghar', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'good', 'got', 'gotten', 'greetings', 'haan', 'had', 'hadd', 'hadn', 'hadnt', \"hadn't\", 'hai', 'hain', 'hamara', 'hamare', 'hamari', 'hamne', 'han', 'happens', 'har', 'hardly', 'has', 'hasn', 'hasnt', \"hasn't\", 'have', 'haven', 'havent', \"haven't\", 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', \"here's\", 'hereupon', 'hers', 'herself', \"he's\", 'hi', 'him', 'himself', 'his', 'hither', 'hm', 'hmm', 'ho', 'hoga', 'hoge', 'hogi', 'hona', 'honaa', 'hone', 'honge', 'hongi', 'honi', 'hopefully', 'hota', 'hotaa', 'hote', 'hoti', 'how', 'howbeit', 'however', 'hoyenge', 'hoyengi', 'hu', 'hua', 'hue', 'huh', 'hui', 'hum', 'humein', 'humne', 'hun', 'huye', 'huyi', 'i', \"i'd\", 'idk', 'ie', 'if', \"i'll\", \"i'm\", 'imo', 'in', 'inasmuch', 'inc', 'inhe', 'inhi', 'inho', 'inka', 'inkaa', 'inke', 'inki', 'inn', 'inner', 'inse', 'insofar', 'into', 'inward', 'is', 'ise', 'isi', 'iska', 'iskaa', 'iske', 'iski', 'isme', 'isn', 'isne', 'isnt', \"isn't\", 'iss', 'isse', 'issi', 'isski', 'it', \"it'd\", \"it'll\", 'itna', 'itne', 'itni', 'itno', 'its', \"it's\", 'itself', 'ityaadi', 'ityadi', \"i've\", 'ja', 'jaa', 'jab', 'jabh', 'jaha', 'jahaan', 'jahan', 'jaisa', 'jaise', 'jaisi', 'jata', 'jayega', 'jidhar', 'jin', 'jinhe', 'jinhi', 'jinho', 'jinhone', 'jinka', 'jinke', 'jinki', 'jinn', 'jis', 'jise', 'jiska', 'jiske', 'jiski', 'jisme', 'jiss', 'jisse', 'jitna', 'jitne', 'jitni', 'jo', 'just', 'jyaada', 'jyada', 'k', 'ka', 'kaafi', 'kab', 'kabhi', 'kafi', 'kaha', 'kahaa', 'kahaan', 'kahan', 'kahi', 'kahin', 'kahte', 'kaisa', 'kaise', 'kaisi', 'kal', 'kam', 'kar', 'kara', 'kare', 'karega', 'karegi', 'karen', 'karenge', 'kari', 'karke', 'karna', 'karne', 'karni', 'karo', 'karta', 'karte', 'karti', 'karu', 'karun', 'karunga', 'karungi', 'kaun', 'kaunsa', 'kayi', 'kch', 'ke', 'keep', 'keeps', 'keh', 'kehte', 'kept', 'khud', 'ki', 'kin', 'kine', 'kinhe', 'kinho', 'kinka', 'kinke', 'kinki', 'kinko', 'kinn', 'kino', 'kis', 'kise', 'kisi', 'kiska', 'kiske', 'kiski', 'kisko', 'kisliye', 'kisne', 'kitna', 'kitne', 'kitni', 'kitno', 'kiya', 'kiye', 'know', 'known', 'knows', 'ko', 'koi', 'kon', 'konsa', 'koyi', 'krna', 'krne', 'kuch', 'kuchch', 'kuchh', 'kul', 'kull', 'kya', 'kyaa', 'kyu', 'kyuki', 'kyun', 'kyunki', 'lagta', 'lagte', 'lagti', 'last', 'lately', 'later', 'le', 'least', 'lekar', 'lekin', 'less', 'lest', 'let', \"let's\", 'li', 'like', 'liked', 'likely', 'little', 'liya', 'liye', 'll', 'lo', 'log', 'logon', 'lol', 'look', 'looking', 'looks', 'ltd', 'lunga', 'm', 'maan', 'maana', 'maane', 'maani', 'maano', 'magar', 'mai', 'main', 'maine', 'mainly', 'mana', 'mane', 'mani', 'mano', 'many', 'mat', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'mein', 'mera', 'mere', 'merely', 'meri', 'might', 'mightn', 'mightnt', \"mightn't\", 'mil', 'mjhe', 'more', 'moreover', 'most', 'mostly', 'much', 'mujhe', 'must', 'mustn', 'mustnt', \"mustn't\", 'my', 'myself', 'na', 'naa', 'naah', 'nahi', 'nahin', 'nai', 'name', 'namely', 'nd', 'ne', 'near', 'nearly', 'necessary', 'neeche', 'need', 'needn', 'neednt', \"needn't\", 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nhi', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nope', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'par', 'pata', 'pe', 'pehla', 'pehle', 'pehli', 'people', 'per', 'perhaps', 'phla', 'phle', 'phli', 'placed', 'please', 'plus', 'poora', 'poori', 'provides', 'pura', 'puri', 'q', 'que', 'quite', 'raha', 'rahaa', 'rahe', 'rahi', 'rakh', 'rakha', 'rakhe', 'rakhen', 'rakhi', 'rakho', 'rather', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'rehte', 'rha', 'rhaa', 'rhe', 'rhi', 'ri', 'right', 's', 'sa', 'saara', 'saare', 'saath', 'sab', 'sabhi', 'sabse', 'sahi', 'said', 'sakta', 'saktaa', 'sakte', 'sakti', 'same', 'sang', 'sara', 'sath', 'saw', 'say', 'saying', 'says', 'se', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'shan', 'shant', \"shan't\", 'she', \"she's\", 'should', 'shouldn', 'shouldnt', \"shouldn't\", \"should've\", 'si', 'since', 'six', 'so', 'soch', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'still', 'sub', 'such', 'sup', 'sure', 't', 'tab', 'tabh', 'tak', 'take', 'taken', 'tarah', 'teen', 'teeno', 'teesra', 'teesre', 'teesri', 'tell', 'tends', 'tera', 'tere', 'teri', 'th', 'tha', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that's\", 'the', 'theek', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', \"there's\", 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'thi', 'thik', 'thing', 'think', 'thinking', 'third', 'this', 'tho', 'thoda', 'thodi', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'three', 'through', 'throughout', 'thru', 'thus', 'tjhe', 'to', 'together', 'toh', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'true', 'truly', 'try', 'trying', 'tu', 'tujhe', 'tum', 'tumhara', 'tumhare', 'tumhari', 'tune', 'twice', 'two', 'um', 'umm', 'un', 'under', 'unhe', 'unhi', 'unho', 'unhone', 'unka', 'unkaa', 'unke', 'unki', 'unko', 'unless', 'unlikely', 'unn', 'unse', 'until', 'unto', 'up', 'upar', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'usi', 'using', 'uska', 'uske', 'usne', 'uss', 'usse', 'ussi', 'usually', 'vaala', 'vaale', 'vaali', 'vahaan', 'vahan', 'vahi', 'vahin', 'vaisa', 'vaise', 'vaisi', 'vala', 'vale', 'vali', 'various', 've', 'very', 'via', 'viz', 'vo', 'waala', 'waale', 'waali', 'wagaira', 'wagairah', 'wagerah', 'waha', 'wahaan', 'wahan', 'wahi', 'wahin', 'waisa', 'waise', 'waisi', 'wala', 'wale', 'wali', 'want', 'wants', 'was', 'wasn', 'wasnt', \"wasn't\", 'way', 'we', \"we'd\", 'well', \"we'll\", 'went', 'were', \"we're\", 'weren', 'werent', \"weren't\", \"we've\", 'what', 'whatever', \"what's\", 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'with', 'within', 'without', 'wo', 'woh', 'wohi', 'won', 'wont', \"won't\", 'would', 'wouldn', 'wouldnt', \"wouldn't\", 'y', 'ya', 'yadi', 'yah', 'yaha', 'yahaan', 'yahan', 'yahi', 'yahin', 'ye', 'yeah', 'yeh', 'yehi', 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'yup']\n",
    "\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(lambda text: cleaning_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98618822",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdda0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "X = df['tweets']\n",
    "y = df['labels']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fddbe4",
   "metadata": {},
   "source": [
    "### Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b6c4bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  82464\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,2) - unigrams and bigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "# Transforming the data using TD-IDF Vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed4515",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eace48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[[315   1  10  19   3   5   8]\n",
      " [ 37 171  16  12   0   1   5]\n",
      " [ 40  17 227  19   1   5   1]\n",
      " [ 55   9  16 252   1   6   7]\n",
      " [ 13   6   8   3  72   1   1]\n",
      " [ 26   5  11  16   1 179   1]\n",
      " [ 52   7   6  20   1   4 141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.87      0.70       361\n",
      "           1       0.79      0.71      0.75       242\n",
      "           2       0.77      0.73      0.75       310\n",
      "           3       0.74      0.73      0.73       346\n",
      "           4       0.91      0.69      0.79       104\n",
      "           5       0.89      0.75      0.81       239\n",
      "           6       0.86      0.61      0.71       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.79      0.73      0.75      1833\n",
      "weighted avg       0.77      0.74      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[[312   1   9  23   3   4   9]\n",
      " [ 34 173  14  13   0   2   6]\n",
      " [ 40  14 230  19   0   6   1]\n",
      " [ 53   7  20 251   1   7   7]\n",
      " [ 12   6   8   3  72   0   3]\n",
      " [ 26   5  11  14   1 181   1]\n",
      " [ 53   7   4  15   2   5 145]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.86      0.70       361\n",
      "           1       0.81      0.71      0.76       242\n",
      "           2       0.78      0.74      0.76       310\n",
      "           3       0.74      0.73      0.73       346\n",
      "           4       0.91      0.69      0.79       104\n",
      "           5       0.88      0.76      0.82       239\n",
      "           6       0.84      0.63      0.72       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.79      0.73      0.75      1833\n",
      "weighted avg       0.77      0.74      0.75      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "RandomForestClassifier()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[[294   9   7  12   3   2  34]\n",
      " [ 21 176  16   9   1   0  19]\n",
      " [ 43  18 221  16   0   1  11]\n",
      " [ 55  17  15 244   0   5  10]\n",
      " [  5   7  11   4  73   1   3]\n",
      " [ 28   9   8  15   1 174   4]\n",
      " [ 40   9   6   6   1   1 168]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.81      0.69       361\n",
      "           1       0.72      0.73      0.72       242\n",
      "           2       0.78      0.71      0.74       310\n",
      "           3       0.80      0.71      0.75       346\n",
      "           4       0.92      0.70      0.80       104\n",
      "           5       0.95      0.73      0.82       239\n",
      "           6       0.67      0.73      0.70       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.78      0.73      0.75      1833\n",
      "weighted avg       0.76      0.74      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "Wall time: 15min 46s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.740316</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistics_regression</td>\n",
       "      <td>0.744135</td>\n",
       "      <td>{'C': 20, 'solver': 'liblinear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.736498</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 90}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  best_score  \\\n",
       "0                   SVC    0.740316   \n",
       "1  logistics_regression    0.744135   \n",
       "2         random_forest    0.736498   \n",
       "\n",
       "                                                    best_params  \n",
       "0                      {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}  \n",
       "1                              {'C': 20, 'solver': 'liblinear'}  \n",
       "2  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 90}  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# implemented GridSearchCV for four models using a loop and a previously created dictionary\n",
    "# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "    print(mp['model'])\n",
    "    print('\\nFitting...')\n",
    "    best_model = clf.fit(X_train, y_train)                      # Training the model\n",
    "    clf_pred = best_model.predict(X_test)                       # Predicting the results\n",
    "    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n",
    "    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n",
    "    scores.append({                                             # Appending results to 'scores' list\n",
    "        'model' : model_name,\n",
    "        'best_score' : best_model.score(X_test, y_test),\n",
    "        'best_params' : clf.best_params_\n",
    "    })\n",
    "    print('\\nScore is appended.\\n')\n",
    "    \n",
    "# Creating data frame with model, best scores and best params:\n",
    "res = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a8150",
   "metadata": {},
   "source": [
    "# 3. Models after removing repeating characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62799e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Code Mixed Hindi-English tweets.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad736baa",
   "metadata": {},
   "source": [
    "### Removing repeating characteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03c6a3d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433466694110834689</td>\n",
       "      <td>[aaj, ka, khel, khatam, hone, k, baad, england, cricket, k, youtube, channel, par, mil, jayegi]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433466691330076675</td>\n",
       "      <td>[purana, manjan, bech, rha, hai]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1433463609858805764</td>\n",
       "      <td>[tumhare, disappointed, se, kuch, ni, hoga]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1433461022900117506</td>\n",
       "      <td>[inse, bas, tiktok, banva, lo, batting, ni, hoti, isse, cricket]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433460862728032257</td>\n",
       "      <td>[bhai, cricket, par, tweets, mat, kara, karo, please, jab, bhi, karte, ho, lanka, lag, jaati, hai, this, makes, me, angry]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>847143219754389504</td>\n",
       "      <td>[gujarati, fraud, kyu, hotey, hai, ?, pnbscam]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9161</th>\n",
       "      <td>909629251409911810</td>\n",
       "      <td>[saala, idhar, 7, lakh, k, car, loan, k, liye, bank, choos, leti, hai, aur, waha, log, crores, ka, fraud, karte, hai, bada, socho, niravmodi, pnbfraud]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9162</th>\n",
       "      <td>909419169644449793</td>\n",
       "      <td>[mitron, nirav, modi, ji, ka, relation, india, k, kaun, se, rich, industrialist, parivaar, se, hai, ?, batao, zara, ?, pnbfraud]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9163</th>\n",
       "      <td>909327295629877248</td>\n",
       "      <td>[or, yahn, ek, month, k, education, loan, ki, emi, pay, ni, karo, to, call, aa, jata, hai, pnbfraud, so, sad]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9164</th>\n",
       "      <td>909322844282089472</td>\n",
       "      <td>[jis, bank, me, 500, rupye, niklwane, k, liye, ghnto, khda, rehna, pdta, hai, unhone, aaj, btaya, k, $, 7, bn, koi, dokhe, se, idhr, udhr, kr, gya, pnb, fraud, this, makes, me, angry]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9165 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id  \\\n",
       "0     1433466694110834689   \n",
       "1     1433466691330076675   \n",
       "2     1433463609858805764   \n",
       "3     1433461022900117506   \n",
       "4     1433460862728032257   \n",
       "...                   ...   \n",
       "9160   847143219754389504   \n",
       "9161   909629251409911810   \n",
       "9162   909419169644449793   \n",
       "9163   909327295629877248   \n",
       "9164   909322844282089472   \n",
       "\n",
       "                                                                                                                                                                                       tweets  \\\n",
       "0                                                                                             [aaj, ka, khel, khatam, hone, k, baad, england, cricket, k, youtube, channel, par, mil, jayegi]   \n",
       "1                                                                                                                                                            [purana, manjan, bech, rha, hai]   \n",
       "2                                                                                                                                                 [tumhare, disappointed, se, kuch, ni, hoga]   \n",
       "3                                                                                                                            [inse, bas, tiktok, banva, lo, batting, ni, hoti, isse, cricket]   \n",
       "4                                                                  [bhai, cricket, par, tweets, mat, kara, karo, please, jab, bhi, karte, ho, lanka, lag, jaati, hai, this, makes, me, angry]   \n",
       "...                                                                                                                                                                                       ...   \n",
       "9160                                                                                                                                           [gujarati, fraud, kyu, hotey, hai, ?, pnbscam]   \n",
       "9161                                  [saala, idhar, 7, lakh, k, car, loan, k, liye, bank, choos, leti, hai, aur, waha, log, crores, ka, fraud, karte, hai, bada, socho, niravmodi, pnbfraud]   \n",
       "9162                                                         [mitron, nirav, modi, ji, ka, relation, india, k, kaun, se, rich, industrialist, parivaar, se, hai, ?, batao, zara, ?, pnbfraud]   \n",
       "9163                                                                            [or, yahn, ek, month, k, education, loan, ki, emi, pay, ni, karo, to, call, aa, jata, hai, pnbfraud, so, sad]   \n",
       "9164  [jis, bank, me, 500, rupye, niklwane, k, liye, ghnto, khda, rehna, pdta, hai, unhone, aaj, btaya, k, $, 7, bn, koi, dokhe, se, idhr, udhr, kr, gya, pnb, fraud, this, makes, me, angry]   \n",
       "\n",
       "      labels  \n",
       "0          0  \n",
       "1          5  \n",
       "2          0  \n",
       "3          3  \n",
       "4          3  \n",
       "...      ...  \n",
       "9160       3  \n",
       "9161       3  \n",
       "9162       3  \n",
       "9163       2  \n",
       "9164       3  \n",
       "\n",
       "[9165 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = (word_tokenize(i) for i in df.tweets)\n",
    "df['tweets'] = df['tweets'].apply(nltk.word_tokenize)\n",
    "#df['tweets'] = df['tweets'].astype(str)\n",
    "\n",
    "pattern = re.compile(r'(.)\\1*')\n",
    "\n",
    "def reduce_sequence_word(word):\n",
    "    return ''.join([match.group()[:2] if len(match.group()) > 2 else match.group() for match in pattern.finditer(word)])\n",
    "\n",
    "def reduce_sequence_tweet(tweet):\n",
    "    return [reduce_sequence_word(word) for word in tweet]\n",
    "\n",
    "df.tweets = df.tweets.apply(lambda tweet: reduce_sequence_tweet(tweet))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d2afa",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccfa4cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "X = df['tweets'].astype(str)\n",
    "y = df['labels'].astype(str)\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c7b64",
   "metadata": {},
   "source": [
    "### Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d95ec611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  114187\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,2) - unigrams and bigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "# Transforming the data using TD-IDF Vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0834ac1",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2847dd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[[298   3   9  26   5   7  13]\n",
      " [ 43 165  14  13   0   4   3]\n",
      " [ 45  10 226  19   0   7   3]\n",
      " [ 52   8  17 247   1  19   2]\n",
      " [ 12   3   4   5  74   3   3]\n",
      " [ 28   2   7  16   1 185   0]\n",
      " [ 43   7   4  13   0   3 161]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.83      0.68       361\n",
      "           1       0.83      0.68      0.75       242\n",
      "           2       0.80      0.73      0.76       310\n",
      "           3       0.73      0.71      0.72       346\n",
      "           4       0.91      0.71      0.80       104\n",
      "           5       0.81      0.77      0.79       239\n",
      "           6       0.87      0.70      0.77       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.79      0.73      0.75      1833\n",
      "weighted avg       0.76      0.74      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[[309   4   7  21   5   2  13]\n",
      " [ 42 165  13  12   0   5   5]\n",
      " [ 46  10 226  21   0   5   2]\n",
      " [ 54   4  18 249   1  17   3]\n",
      " [ 16   3   4   4  73   2   2]\n",
      " [ 29   2   6  18   1 183   0]\n",
      " [ 49   4   2  10   0   4 162]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.86      0.68       361\n",
      "           1       0.86      0.68      0.76       242\n",
      "           2       0.82      0.73      0.77       310\n",
      "           3       0.74      0.72      0.73       346\n",
      "           4       0.91      0.70      0.79       104\n",
      "           5       0.84      0.77      0.80       239\n",
      "           6       0.87      0.70      0.78       231\n",
      "\n",
      "    accuracy                           0.75      1833\n",
      "   macro avg       0.80      0.74      0.76      1833\n",
      "weighted avg       0.77      0.75      0.75      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "RandomForestClassifier()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[[328   6   3   7   4   0  13]\n",
      " [ 60 172   4   3   0   0   3]\n",
      " [ 81  16 204   6   1   0   2]\n",
      " [106  10   9 210   2   2   7]\n",
      " [ 20   3   3   0  76   0   2]\n",
      " [ 58   9   6   6   3 153   4]\n",
      " [ 47  12   2   2   2   1 165]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.91      0.62       361\n",
      "           1       0.75      0.71      0.73       242\n",
      "           2       0.88      0.66      0.75       310\n",
      "           3       0.90      0.61      0.72       346\n",
      "           4       0.86      0.73      0.79       104\n",
      "           5       0.98      0.64      0.77       239\n",
      "           6       0.84      0.71      0.77       231\n",
      "\n",
      "    accuracy                           0.71      1833\n",
      "   macro avg       0.81      0.71      0.74      1833\n",
      "weighted avg       0.79      0.71      0.73      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "Wall time: 21min 13s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.739771</td>\n",
       "      <td>{'C': 10, 'gamma': 1, 'kernel': 'sigmoid'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistics_regression</td>\n",
       "      <td>0.745772</td>\n",
       "      <td>{'C': 100, 'solver': 'lbfgs'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.713584</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 95}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  best_score  \\\n",
       "0                   SVC    0.739771   \n",
       "1  logistics_regression    0.745772   \n",
       "2         random_forest    0.713584   \n",
       "\n",
       "                                                    best_params  \n",
       "0                    {'C': 10, 'gamma': 1, 'kernel': 'sigmoid'}  \n",
       "1                                 {'C': 100, 'solver': 'lbfgs'}  \n",
       "2  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 95}  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# implemented GridSearchCV for four models using a loop and a previously created dictionary\n",
    "# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "    print(mp['model'])\n",
    "    print('\\nFitting...')\n",
    "    best_model = clf.fit(X_train, y_train)                      # Training the model\n",
    "    clf_pred = best_model.predict(X_test)                       # Predicting the results\n",
    "    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n",
    "    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n",
    "    scores.append({                                             # Appending results to 'scores' list\n",
    "        'model' : model_name,\n",
    "        'best_score' : best_model.score(X_test, y_test),\n",
    "        'best_params' : clf.best_params_\n",
    "    })\n",
    "    print('\\nScore is appended.\\n')\n",
    "    \n",
    "# Creating data frame with model, best scores and best params:\n",
    "res = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d50191e",
   "metadata": {},
   "source": [
    "# 4. Models after removing Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc4782c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Code Mixed Hindi-English tweets.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e02b9e",
   "metadata": {},
   "source": [
    "### Removing Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ce4c8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0766348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations_list = string.punctuation\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(lambda x: cleaning_punctuations(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5904658",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7534f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "X = df['tweets']\n",
    "y = df['labels']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b5927",
   "metadata": {},
   "source": [
    "### Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "721f82ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  114384\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,2) - unigrams and bigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "# Transforming the data using TD-IDF Vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819788e7",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "087adc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[[299   3   9  26   5   6  13]\n",
      " [ 43 165  14  13   0   4   3]\n",
      " [ 43  10 227  20   0   7   3]\n",
      " [ 49  10  18 248   1  18   2]\n",
      " [ 12   3   4   5  74   3   3]\n",
      " [ 27   1   9  16   1 184   1]\n",
      " [ 41   8   5  13   1   3 160]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.83      0.68       361\n",
      "           1       0.82      0.68      0.75       242\n",
      "           2       0.79      0.73      0.76       310\n",
      "           3       0.73      0.72      0.72       346\n",
      "           4       0.90      0.71      0.80       104\n",
      "           5       0.82      0.77      0.79       239\n",
      "           6       0.86      0.69      0.77       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.79      0.73      0.75      1833\n",
      "weighted avg       0.76      0.74      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[[312   2   6  21   5   3  12]\n",
      " [ 41 163  13  15   0   5   5]\n",
      " [ 47  10 223  23   0   5   2]\n",
      " [ 53   4  17 249   1  19   3]\n",
      " [ 15   3   4   4  74   2   2]\n",
      " [ 30   1   5  19   1 182   1]\n",
      " [ 47   4   4  12   1   4 159]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.86      0.69       361\n",
      "           1       0.87      0.67      0.76       242\n",
      "           2       0.82      0.72      0.77       310\n",
      "           3       0.73      0.72      0.72       346\n",
      "           4       0.90      0.71      0.80       104\n",
      "           5       0.83      0.76      0.79       239\n",
      "           6       0.86      0.69      0.77       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.80      0.73      0.76      1833\n",
      "weighted avg       0.77      0.74      0.75      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "RandomForestClassifier()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[[327   6   5   5   3   0  15]\n",
      " [ 58 170   5   3   0   0   6]\n",
      " [ 90  14 196   6   1   2   1]\n",
      " [107   9   4 211   0   9   6]\n",
      " [ 26   1   0   0  72   3   2]\n",
      " [ 53   5   7   9   1 160   4]\n",
      " [ 45  12   3   1   0   0 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.91      0.61       361\n",
      "           1       0.78      0.70      0.74       242\n",
      "           2       0.89      0.63      0.74       310\n",
      "           3       0.90      0.61      0.73       346\n",
      "           4       0.94      0.69      0.80       104\n",
      "           5       0.92      0.67      0.77       239\n",
      "           6       0.83      0.74      0.78       231\n",
      "\n",
      "    accuracy                           0.71      1833\n",
      "   macro avg       0.82      0.71      0.74      1833\n",
      "weighted avg       0.79      0.71      0.73      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "Wall time: 42min 9s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.740316</td>\n",
       "      <td>{'C': 10, 'gamma': 1, 'kernel': 'sigmoid'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistics_regression</td>\n",
       "      <td>0.743044</td>\n",
       "      <td>{'C': 100, 'solver': 'lbfgs'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.712493</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 85}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  best_score  \\\n",
       "0                   SVC    0.740316   \n",
       "1  logistics_regression    0.743044   \n",
       "2         random_forest    0.712493   \n",
       "\n",
       "                                                    best_params  \n",
       "0                    {'C': 10, 'gamma': 1, 'kernel': 'sigmoid'}  \n",
       "1                                 {'C': 100, 'solver': 'lbfgs'}  \n",
       "2  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 85}  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# implemented GridSearchCV for four models using a loop and a previously created dictionary\n",
    "# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n",
    "\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "    print(mp['model'])\n",
    "    print('\\nFitting...')\n",
    "    best_model = clf.fit(X_train, y_train)                      # Training the model\n",
    "    clf_pred = best_model.predict(X_test)                       # Predicting the results\n",
    "    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n",
    "    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n",
    "    scores.append({                                             # Appending results to 'scores' list\n",
    "        'model' : model_name,\n",
    "        'best_score' : best_model.score(X_test, y_test),\n",
    "        'best_params' : clf.best_params_\n",
    "    })\n",
    "    print('\\nScore is appended.\\n')\n",
    "    \n",
    "# Creating data frame with model, best scores and best params:\n",
    "res = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70830fe",
   "metadata": {},
   "source": [
    "# 5. Models after removing numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71d19273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Code Mixed Hindi-English tweets.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eabdcf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(text):\n",
    "    return re.sub('[0-9]+', '', text)\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(lambda text: cleaning_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f586c",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de09433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "X = df['tweets']\n",
    "y = df['labels']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05026c",
   "metadata": {},
   "source": [
    "### Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc52ca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  113273\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,2) - unigrams and bigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "# Transforming the data using TD-IDF Vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ccb6f8",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47a93fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[[297   3   9  27   5   7  13]\n",
      " [ 43 165  14  13   0   4   3]\n",
      " [ 41  10 229  20   0   7   3]\n",
      " [ 52   8  17 247   1  19   2]\n",
      " [ 12   3   4   5  74   3   3]\n",
      " [ 27   2   8  15   1 185   1]\n",
      " [ 42   7   5  13   1   3 160]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.82      0.68       361\n",
      "           1       0.83      0.68      0.75       242\n",
      "           2       0.80      0.74      0.77       310\n",
      "           3       0.73      0.71      0.72       346\n",
      "           4       0.90      0.71      0.80       104\n",
      "           5       0.81      0.77      0.79       239\n",
      "           6       0.86      0.69      0.77       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.79      0.73      0.75      1833\n",
      "weighted avg       0.76      0.74      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[[312   3   7  20   5   2  12]\n",
      " [ 41 167  15  10   0   3   6]\n",
      " [ 45   9 226  24   0   5   1]\n",
      " [ 55   4  18 248   1  17   3]\n",
      " [ 16   3   4   4  73   2   2]\n",
      " [ 30   1   4  19   1 183   1]\n",
      " [ 47   4   4   9   1   4 162]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.86      0.69       361\n",
      "           1       0.87      0.69      0.77       242\n",
      "           2       0.81      0.73      0.77       310\n",
      "           3       0.74      0.72      0.73       346\n",
      "           4       0.90      0.70      0.79       104\n",
      "           5       0.85      0.77      0.80       239\n",
      "           6       0.87      0.70      0.78       231\n",
      "\n",
      "    accuracy                           0.75      1833\n",
      "   macro avg       0.80      0.74      0.76      1833\n",
      "weighted avg       0.78      0.75      0.75      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "RandomForestClassifier()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[[331   7   4   4   4   0  11]\n",
      " [ 53 171   9   2   0   0   7]\n",
      " [ 77  21 202   4   1   0   5]\n",
      " [108   6  11 212   2   1   6]\n",
      " [ 24   1   0   1  75   1   2]\n",
      " [ 56   4   4   5   1 166   3]\n",
      " [ 46  10   5   1   0   0 169]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.92      0.63       361\n",
      "           1       0.78      0.71      0.74       242\n",
      "           2       0.86      0.65      0.74       310\n",
      "           3       0.93      0.61      0.74       346\n",
      "           4       0.90      0.72      0.80       104\n",
      "           5       0.99      0.69      0.82       239\n",
      "           6       0.83      0.73      0.78       231\n",
      "\n",
      "    accuracy                           0.72      1833\n",
      "   macro avg       0.82      0.72      0.75      1833\n",
      "weighted avg       0.80      0.72      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "Wall time: 45min 24s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.740316</td>\n",
       "      <td>{'C': 100, 'gamma': 1, 'kernel': 'sigmoid'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistics_regression</td>\n",
       "      <td>0.747954</td>\n",
       "      <td>{'C': 100, 'solver': 'lbfgs'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 100}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  best_score  \\\n",
       "0                   SVC    0.740316   \n",
       "1  logistics_regression    0.747954   \n",
       "2         random_forest    0.723404   \n",
       "\n",
       "                                                     best_params  \n",
       "0                    {'C': 100, 'gamma': 1, 'kernel': 'sigmoid'}  \n",
       "1                                  {'C': 100, 'solver': 'lbfgs'}  \n",
       "2  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 100}  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# implemented GridSearchCV for four models using a loop and a previously created dictionary\n",
    "# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "    print(mp['model'])\n",
    "    print('\\nFitting...')\n",
    "    best_model = clf.fit(X_train, y_train)                      # Training the model\n",
    "    clf_pred = best_model.predict(X_test)                       # Predicting the results\n",
    "    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n",
    "    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n",
    "    scores.append({                                             # Appending results to 'scores' list\n",
    "        'model' : model_name,\n",
    "        'best_score' : best_model.score(X_test, y_test),\n",
    "        'best_params' : clf.best_params_\n",
    "    })\n",
    "    print('\\nScore is appended.\\n')\n",
    "    \n",
    "# Creating data frame with model, best scores and best params:\n",
    "res = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3a5e5",
   "metadata": {},
   "source": [
    "# 6. Models after removing all the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d25f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Code Mixed Hindi-English tweets.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df4102c",
   "metadata": {},
   "source": [
    "### Removing Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c33fe31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bcea520",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations_list = string.punctuation\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(lambda x: cleaning_punctuations(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09674035",
   "metadata": {},
   "source": [
    "### Removing Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4210346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hinglish stopwords which contains 1036 words from both English and Hindi languages\n",
    "# Source: https://github.com/TrigonaMinima/HinglishNLP/blob/master/data/assets/stop_hinglish\n",
    "\n",
    "stopwordlist = ['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab', 'abbe', 'abbey', 'abe', 'abhi', 'able', 'about', 'above', 'accha', 'according', 'accordingly', 'acha', 'achcha', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'agar', 'ain', 'aint', \"ain't\", 'aisa', 'aise', 'aisi', 'alag', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'andar', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'ap', 'apan', 'apart', 'apna', 'apnaa', 'apne', 'apni', 'appear', 'are', 'aren', 'arent', \"aren't\", 'around', 'arre', 'as', 'aside', 'ask', 'asking', 'at', 'aur', 'avum', 'aya', 'aye', 'baad', 'baar', 'bad', 'bahut', 'bana', 'banae', 'banai', 'banao', 'banaya', 'banaye', 'banayi', 'banda', 'bande', 'bandi', 'bane', 'bani', 'bas', 'bata', 'batao', 'bc', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'bhai', 'bheetar', 'bhi', 'bhitar', 'bht', 'bilkul', 'bohot', 'bol', 'bola', 'bole', 'boli', 'bolo', 'bolta', 'bolte', 'bolti', 'both', 'brief', 'bro', 'btw', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain', 'certainly', 'chahiye', 'chaiye', 'chal', 'chalega', 'chhaiye', 'clearly', \"c'mon\", 'com', 'come', 'comes', 'could', 'couldn', 'couldnt', \"couldn't\", 'd', 'de', 'dede', 'dega', 'degi', 'dekh', 'dekha', 'dekhe', 'dekhi', 'dekho', 'denge', 'dhang', 'di', 'did', 'didn', 'didnt', \"didn't\", 'dijiye', 'diya', 'diyaa', 'diye', 'diyo', 'do', 'does', 'doesn', 'doesnt', \"doesn't\", 'doing', 'done', 'dono', 'dont', \"don't\", 'doosra', 'doosre', 'down', 'downwards', 'dude', 'dunga', 'dungi', 'during', 'dusra', 'dusre', 'dusri', 'dvaara', 'dvara', 'dwaara', 'dwara', 'each', 'edu', 'eg', 'eight', 'either', 'ek', 'else', 'elsewhere', 'enough', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'fir', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forth', 'four', 'from', 'further', 'furthermore', 'gaya', 'gaye', 'gayi', 'get', 'gets', 'getting', 'ghar', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'good', 'got', 'gotten', 'greetings', 'haan', 'had', 'hadd', 'hadn', 'hadnt', \"hadn't\", 'hai', 'hain', 'hamara', 'hamare', 'hamari', 'hamne', 'han', 'happens', 'har', 'hardly', 'has', 'hasn', 'hasnt', \"hasn't\", 'have', 'haven', 'havent', \"haven't\", 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', \"here's\", 'hereupon', 'hers', 'herself', \"he's\", 'hi', 'him', 'himself', 'his', 'hither', 'hm', 'hmm', 'ho', 'hoga', 'hoge', 'hogi', 'hona', 'honaa', 'hone', 'honge', 'hongi', 'honi', 'hopefully', 'hota', 'hotaa', 'hote', 'hoti', 'how', 'howbeit', 'however', 'hoyenge', 'hoyengi', 'hu', 'hua', 'hue', 'huh', 'hui', 'hum', 'humein', 'humne', 'hun', 'huye', 'huyi', 'i', \"i'd\", 'idk', 'ie', 'if', \"i'll\", \"i'm\", 'imo', 'in', 'inasmuch', 'inc', 'inhe', 'inhi', 'inho', 'inka', 'inkaa', 'inke', 'inki', 'inn', 'inner', 'inse', 'insofar', 'into', 'inward', 'is', 'ise', 'isi', 'iska', 'iskaa', 'iske', 'iski', 'isme', 'isn', 'isne', 'isnt', \"isn't\", 'iss', 'isse', 'issi', 'isski', 'it', \"it'd\", \"it'll\", 'itna', 'itne', 'itni', 'itno', 'its', \"it's\", 'itself', 'ityaadi', 'ityadi', \"i've\", 'ja', 'jaa', 'jab', 'jabh', 'jaha', 'jahaan', 'jahan', 'jaisa', 'jaise', 'jaisi', 'jata', 'jayega', 'jidhar', 'jin', 'jinhe', 'jinhi', 'jinho', 'jinhone', 'jinka', 'jinke', 'jinki', 'jinn', 'jis', 'jise', 'jiska', 'jiske', 'jiski', 'jisme', 'jiss', 'jisse', 'jitna', 'jitne', 'jitni', 'jo', 'just', 'jyaada', 'jyada', 'k', 'ka', 'kaafi', 'kab', 'kabhi', 'kafi', 'kaha', 'kahaa', 'kahaan', 'kahan', 'kahi', 'kahin', 'kahte', 'kaisa', 'kaise', 'kaisi', 'kal', 'kam', 'kar', 'kara', 'kare', 'karega', 'karegi', 'karen', 'karenge', 'kari', 'karke', 'karna', 'karne', 'karni', 'karo', 'karta', 'karte', 'karti', 'karu', 'karun', 'karunga', 'karungi', 'kaun', 'kaunsa', 'kayi', 'kch', 'ke', 'keep', 'keeps', 'keh', 'kehte', 'kept', 'khud', 'ki', 'kin', 'kine', 'kinhe', 'kinho', 'kinka', 'kinke', 'kinki', 'kinko', 'kinn', 'kino', 'kis', 'kise', 'kisi', 'kiska', 'kiske', 'kiski', 'kisko', 'kisliye', 'kisne', 'kitna', 'kitne', 'kitni', 'kitno', 'kiya', 'kiye', 'know', 'known', 'knows', 'ko', 'koi', 'kon', 'konsa', 'koyi', 'krna', 'krne', 'kuch', 'kuchch', 'kuchh', 'kul', 'kull', 'kya', 'kyaa', 'kyu', 'kyuki', 'kyun', 'kyunki', 'lagta', 'lagte', 'lagti', 'last', 'lately', 'later', 'le', 'least', 'lekar', 'lekin', 'less', 'lest', 'let', \"let's\", 'li', 'like', 'liked', 'likely', 'little', 'liya', 'liye', 'll', 'lo', 'log', 'logon', 'lol', 'look', 'looking', 'looks', 'ltd', 'lunga', 'm', 'maan', 'maana', 'maane', 'maani', 'maano', 'magar', 'mai', 'main', 'maine', 'mainly', 'mana', 'mane', 'mani', 'mano', 'many', 'mat', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'mein', 'mera', 'mere', 'merely', 'meri', 'might', 'mightn', 'mightnt', \"mightn't\", 'mil', 'mjhe', 'more', 'moreover', 'most', 'mostly', 'much', 'mujhe', 'must', 'mustn', 'mustnt', \"mustn't\", 'my', 'myself', 'na', 'naa', 'naah', 'nahi', 'nahin', 'nai', 'name', 'namely', 'nd', 'ne', 'near', 'nearly', 'necessary', 'neeche', 'need', 'needn', 'neednt', \"needn't\", 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nhi', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nope', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'par', 'pata', 'pe', 'pehla', 'pehle', 'pehli', 'people', 'per', 'perhaps', 'phla', 'phle', 'phli', 'placed', 'please', 'plus', 'poora', 'poori', 'provides', 'pura', 'puri', 'q', 'que', 'quite', 'raha', 'rahaa', 'rahe', 'rahi', 'rakh', 'rakha', 'rakhe', 'rakhen', 'rakhi', 'rakho', 'rather', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'rehte', 'rha', 'rhaa', 'rhe', 'rhi', 'ri', 'right', 's', 'sa', 'saara', 'saare', 'saath', 'sab', 'sabhi', 'sabse', 'sahi', 'said', 'sakta', 'saktaa', 'sakte', 'sakti', 'same', 'sang', 'sara', 'sath', 'saw', 'say', 'saying', 'says', 'se', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'shan', 'shant', \"shan't\", 'she', \"she's\", 'should', 'shouldn', 'shouldnt', \"shouldn't\", \"should've\", 'si', 'since', 'six', 'so', 'soch', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'still', 'sub', 'such', 'sup', 'sure', 't', 'tab', 'tabh', 'tak', 'take', 'taken', 'tarah', 'teen', 'teeno', 'teesra', 'teesre', 'teesri', 'tell', 'tends', 'tera', 'tere', 'teri', 'th', 'tha', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that's\", 'the', 'theek', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', \"there's\", 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'thi', 'thik', 'thing', 'think', 'thinking', 'third', 'this', 'tho', 'thoda', 'thodi', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'three', 'through', 'throughout', 'thru', 'thus', 'tjhe', 'to', 'together', 'toh', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'true', 'truly', 'try', 'trying', 'tu', 'tujhe', 'tum', 'tumhara', 'tumhare', 'tumhari', 'tune', 'twice', 'two', 'um', 'umm', 'un', 'under', 'unhe', 'unhi', 'unho', 'unhone', 'unka', 'unkaa', 'unke', 'unki', 'unko', 'unless', 'unlikely', 'unn', 'unse', 'until', 'unto', 'up', 'upar', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'usi', 'using', 'uska', 'uske', 'usne', 'uss', 'usse', 'ussi', 'usually', 'vaala', 'vaale', 'vaali', 'vahaan', 'vahan', 'vahi', 'vahin', 'vaisa', 'vaise', 'vaisi', 'vala', 'vale', 'vali', 'various', 've', 'very', 'via', 'viz', 'vo', 'waala', 'waale', 'waali', 'wagaira', 'wagairah', 'wagerah', 'waha', 'wahaan', 'wahan', 'wahi', 'wahin', 'waisa', 'waise', 'waisi', 'wala', 'wale', 'wali', 'want', 'wants', 'was', 'wasn', 'wasnt', \"wasn't\", 'way', 'we', \"we'd\", 'well', \"we'll\", 'went', 'were', \"we're\", 'weren', 'werent', \"weren't\", \"we've\", 'what', 'whatever', \"what's\", 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'with', 'within', 'without', 'wo', 'woh', 'wohi', 'won', 'wont', \"won't\", 'would', 'wouldn', 'wouldnt', \"wouldn't\", 'y', 'ya', 'yadi', 'yah', 'yaha', 'yahaan', 'yahan', 'yahi', 'yahin', 'ye', 'yeah', 'yeh', 'yehi', 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'yup']\n",
    "\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(lambda text: cleaning_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1ce15",
   "metadata": {},
   "source": [
    "### Removing Numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ee85548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(text):\n",
    "    return re.sub('[0-9]+', '', text)\n",
    "\n",
    "df['tweets'] = df['tweets'].apply(lambda text: cleaning_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d0578",
   "metadata": {},
   "source": [
    "### Removing repeating characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5ea5936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433466694110834689</td>\n",
       "      <td>[khel, khatam, england, cricket, youtube, channel, jayegi]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433466691330076675</td>\n",
       "      <td>[purana, manjan, bech]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1433463609858805764</td>\n",
       "      <td>[disappointed, ni]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1433461022900117506</td>\n",
       "      <td>[tiktok, banva, batting, ni, cricket]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433460862728032257</td>\n",
       "      <td>[cricket, tweets, lanka, lag, jaati, makes, angry]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>847143219754389504</td>\n",
       "      <td>[gujarati, fraud, hotey, pnbscam]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9161</th>\n",
       "      <td>909629251409911810</td>\n",
       "      <td>[saala, idhar, lakh, car, loan, bank, choos, leti, crores, fraud, bada, socho, niravmodi, pnbfraud]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9162</th>\n",
       "      <td>909419169644449793</td>\n",
       "      <td>[mitron, nirav, modi, ji, relation, india, rich, industrialist, parivaar, zara, pnbfraud]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9163</th>\n",
       "      <td>909327295629877248</td>\n",
       "      <td>[yahn, month, education, loan, emi, pay, ni, call, aa, pnbfraud, sad]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9164</th>\n",
       "      <td>909322844282089472</td>\n",
       "      <td>[bank, rupye, niklwane, ghnto, khda, rehna, pdta, btaya, bn, dokhe, idhr, udhr, kr, gya, pnb, fraud, makes, angry]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9165 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id  \\\n",
       "0     1433466694110834689   \n",
       "1     1433466691330076675   \n",
       "2     1433463609858805764   \n",
       "3     1433461022900117506   \n",
       "4     1433460862728032257   \n",
       "...                   ...   \n",
       "9160   847143219754389504   \n",
       "9161   909629251409911810   \n",
       "9162   909419169644449793   \n",
       "9163   909327295629877248   \n",
       "9164   909322844282089472   \n",
       "\n",
       "                                                                                                                  tweets  \\\n",
       "0                                                             [khel, khatam, england, cricket, youtube, channel, jayegi]   \n",
       "1                                                                                                 [purana, manjan, bech]   \n",
       "2                                                                                                     [disappointed, ni]   \n",
       "3                                                                                  [tiktok, banva, batting, ni, cricket]   \n",
       "4                                                                     [cricket, tweets, lanka, lag, jaati, makes, angry]   \n",
       "...                                                                                                                  ...   \n",
       "9160                                                                                   [gujarati, fraud, hotey, pnbscam]   \n",
       "9161                 [saala, idhar, lakh, car, loan, bank, choos, leti, crores, fraud, bada, socho, niravmodi, pnbfraud]   \n",
       "9162                           [mitron, nirav, modi, ji, relation, india, rich, industrialist, parivaar, zara, pnbfraud]   \n",
       "9163                                               [yahn, month, education, loan, emi, pay, ni, call, aa, pnbfraud, sad]   \n",
       "9164  [bank, rupye, niklwane, ghnto, khda, rehna, pdta, btaya, bn, dokhe, idhr, udhr, kr, gya, pnb, fraud, makes, angry]   \n",
       "\n",
       "      labels  \n",
       "0          0  \n",
       "1          5  \n",
       "2          0  \n",
       "3          3  \n",
       "4          3  \n",
       "...      ...  \n",
       "9160       3  \n",
       "9161       3  \n",
       "9162       3  \n",
       "9163       2  \n",
       "9164       3  \n",
       "\n",
       "[9165 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = (word_tokenize(i) for i in df.tweets)\n",
    "df['tweets'] = df['tweets'].apply(nltk.word_tokenize)\n",
    "#df['tweets'] = df['tweets'].astype(str)\n",
    "\n",
    "pattern = re.compile(r'(.)\\1*')\n",
    "\n",
    "def reduce_sequence_word(word):\n",
    "    return ''.join([match.group()[:2] if len(match.group()) > 2 else match.group() for match in pattern.finditer(word)])\n",
    "\n",
    "def reduce_sequence_tweet(tweet):\n",
    "    return [reduce_sequence_word(word) for word in tweet]\n",
    "\n",
    "df.tweets = df.tweets.apply(lambda tweet: reduce_sequence_tweet(tweet))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92ecca",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0577b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Test sets\n",
    "X = df['tweets'].astype(str)\n",
    "y = df['labels'].astype(str)\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f096c59",
   "metadata": {},
   "source": [
    "### Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "413c0b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  81080\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,2) - unigrams and bigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "# Transforming the data using TD-IDF Vectorizer\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0b53c8",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ac506ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[[313   2  12  20   3   5   6]\n",
      " [ 39 169  16  11   1   2   4]\n",
      " [ 42  15 227  20   1   5   0]\n",
      " [ 54  10  16 252   2   5   7]\n",
      " [ 12   6   8   3  73   1   1]\n",
      " [ 27   5  12  14   1 179   1]\n",
      " [ 58   8   5  19   1   3 137]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.87      0.69       361\n",
      "           1       0.79      0.70      0.74       242\n",
      "           2       0.77      0.73      0.75       310\n",
      "           3       0.74      0.73      0.74       346\n",
      "           4       0.89      0.70      0.78       104\n",
      "           5       0.90      0.75      0.82       239\n",
      "           6       0.88      0.59      0.71       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.79      0.72      0.75      1833\n",
      "weighted avg       0.76      0.74      0.74      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[[314   1   8  23   3   4   8]\n",
      " [ 31 174  14  14   1   2   6]\n",
      " [ 43  14 226  21   0   5   1]\n",
      " [ 52   8  20 252   1   6   7]\n",
      " [ 12   6   8   3  72   0   3]\n",
      " [ 26   5  11  13   1 182   1]\n",
      " [ 57   7   3  15   1   4 144]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.87      0.70       361\n",
      "           1       0.81      0.72      0.76       242\n",
      "           2       0.78      0.73      0.75       310\n",
      "           3       0.74      0.73      0.73       346\n",
      "           4       0.91      0.69      0.79       104\n",
      "           5       0.90      0.76      0.82       239\n",
      "           6       0.85      0.62      0.72       231\n",
      "\n",
      "    accuracy                           0.74      1833\n",
      "   macro avg       0.80      0.73      0.75      1833\n",
      "weighted avg       0.77      0.74      0.75      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "RandomForestClassifier()\n",
      "\n",
      "Fitting...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[[300   6   4  13   3   1  34]\n",
      " [ 26 170  16   9   0   0  21]\n",
      " [ 47  18 216  13   0   0  16]\n",
      " [ 60  22  16 235   0   3  10]\n",
      " [  8   7  10   2  75   0   2]\n",
      " [ 31  10   7  11   1 173   6]\n",
      " [ 41   9   8   5   1   1 166]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.83      0.69       361\n",
      "           1       0.70      0.70      0.70       242\n",
      "           2       0.78      0.70      0.74       310\n",
      "           3       0.82      0.68      0.74       346\n",
      "           4       0.94      0.72      0.82       104\n",
      "           5       0.97      0.72      0.83       239\n",
      "           6       0.65      0.72      0.68       231\n",
      "\n",
      "    accuracy                           0.73      1833\n",
      "   macro avg       0.78      0.72      0.74      1833\n",
      "weighted avg       0.76      0.73      0.73      1833\n",
      "\n",
      "\n",
      "Score is appended.\n",
      "\n",
      "Wall time: 34min 20s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.736498</td>\n",
       "      <td>{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistics_regression</td>\n",
       "      <td>0.744135</td>\n",
       "      <td>{'C': 20, 'solver': 'liblinear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.728314</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None, 'n_estimators': 100}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  best_score  \\\n",
       "0                   SVC    0.736498   \n",
       "1  logistics_regression    0.744135   \n",
       "2         random_forest    0.728314   \n",
       "\n",
       "                                                     best_params  \n",
       "0                     {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}  \n",
       "1                               {'C': 20, 'solver': 'liblinear'}  \n",
       "2  {'criterion': 'gini', 'max_depth': None, 'n_estimators': 100}  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# implemented GridSearchCV for four models using a loop and a previously created dictionary\n",
    "# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "    print(mp['model'])\n",
    "    print('\\nFitting...')\n",
    "    best_model = clf.fit(X_train, y_train)                      # Training the model\n",
    "    clf_pred = best_model.predict(X_test)                       # Predicting the results\n",
    "    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n",
    "    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n",
    "    scores.append({                                             # Appending results to 'scores' list\n",
    "        'model' : model_name,\n",
    "        'best_score' : best_model.score(X_test, y_test),\n",
    "        'best_params' : clf.best_params_\n",
    "    })\n",
    "    print('\\nScore is appended.\\n')\n",
    "    \n",
    "# Creating data frame with model, best scores and best params:\n",
    "res = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97717dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
